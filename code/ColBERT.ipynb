{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colbert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from nltk import tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  is_sarcastic\n",
       "0  thirtysomething scientists unveil doomsday clo...           1.0\n",
       "1  dem rep. totally nails why congress is falling...           0.0\n",
       "2  eat your veggies: 9 deliciously different recipes           0.0\n",
       "3  inclement weather prevents liar from getting t...           1.0\n",
       "4  mother comes pretty close to using word 'strea...           1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "os.chdir(\"../datasets/\")\n",
    "#dataset_path = os.path.abspath(os.curdir)\n",
    "data = pd.read_parquet(\"combined.parquet\")\n",
    "\n",
    "# Display the first few rows of the dataset for a quick overview\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sentence        0\n",
       " is_sarcastic    0\n",
       " dtype: int64,\n",
       " is_sarcastic\n",
       " 0.0    0.521391\n",
       " 1.0    0.478609\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any null values in the dataset\n",
    "null_check = data.isnull().sum()\n",
    "\n",
    "# Checking the distribution of the 'is_sarcastic' column\n",
    "label_distribution = data[\"is_sarcastic\"].value_counts(normalize=True)\n",
    "\n",
    "null_check, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27828, 5963, 5964)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data cleaning: removing special characters and escape sequences from the sentences\n",
    "data[\"sentence\"] = data[\"sentence\"].apply(lambda x: re.sub(r\"[\\n\\r\\t]+\", \" \", x))\n",
    "\n",
    "# Max sentences to take into account\n",
    "MAX_SENTENCES = 10\n",
    "\n",
    "# Dropping the sample that are too long\n",
    "count_sentences = 0\n",
    "idx =[]\n",
    "for i in range(len(data[\"sentence\"])):\n",
    "    sentence = str(data[\"sentence\"][i])\n",
    "    Splitted_sentence = tokenize.sent_tokenize(sentence)\n",
    "    l = len(Splitted_sentence)\n",
    "    if(l>MAX_SENTENCES):\n",
    "        idx.append(i)\n",
    "    count_sentences = max(count_sentences, l)\n",
    "\n",
    "\n",
    "data = data.drop(idx)\n",
    "\n",
    "# Splitting the dataset into training, validation, and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Showing the size of each split\n",
    "train_size, val_size, test_size = len(train_data), len(val_data), len(test_data)\n",
    "train_size, val_size, test_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "class SarcasticSentencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for the sarcastic sentences dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, labels, tokenizer, MAX_SENTENCES):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.MAX_SENTENCES = MAX_SENTENCES\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        Splitted_sentence = tokenize.sent_tokenize(sentence)\n",
    "        label = self.labels[item]\n",
    "\n",
    "    # Encoding the full sentence using the tokenizer\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=60,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids_list = [encoding[\"input_ids\"].flatten()]\n",
    "        attention_list = [encoding[\"attention_mask\"].flatten()]\n",
    "\n",
    "        # Encoding each sentences individually\n",
    "        for s in Splitted_sentence:\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                s,\n",
    "                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                max_length=20,\n",
    "                return_token_type_ids=False,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "                truncation=True,\n",
    "            )\n",
    "            input_ids_list.append(encoding[\"input_ids\"].flatten())\n",
    "            attention_list.append(encoding[\"attention_mask\"].flatten())\n",
    "        \n",
    "        input_ids_tensor = torch.cat(input_ids_list)\n",
    "        attention_tensor = torch.cat(attention_list)\n",
    "\n",
    "        size_to_extend = self.MAX_SENTENCES*20 + 60 - input_ids_tensor.size()[0]\n",
    "\n",
    "        input_ids_tensor = torch.cat([input_ids_tensor, torch.zeros(size_to_extend)])\n",
    "        attention_tensor = torch.cat([attention_tensor, torch.zeros(size_to_extend)])\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"sentence\": sentence,\n",
    "            \"input_ids\": input_ids_tensor,\n",
    "            \"attention_mask\": attention_tensor,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 128  # Maximum length of the tokens list\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Creating instances of the SarcasticSentencesDataset\n",
    "train_dataset = SarcasticSentencesDataset(\n",
    "    train_data[\"sentence\"].to_numpy(),\n",
    "    train_data[\"is_sarcastic\"].to_numpy(),\n",
    "    tokenizer,\n",
    "    MAX_SENTENCES,\n",
    ")\n",
    "\n",
    "val_dataset = SarcasticSentencesDataset(\n",
    "    val_data[\"sentence\"].to_numpy(),\n",
    "    val_data[\"is_sarcastic\"].to_numpy(),\n",
    "    tokenizer,\n",
    "    MAX_SENTENCES,\n",
    ")\n",
    "\n",
    "test_dataset = SarcasticSentencesDataset(\n",
    "    test_data[\"sentence\"].to_numpy(),\n",
    "    test_data[\"is_sarcastic\"].to_numpy(),\n",
    "    tokenizer,\n",
    "    MAX_SENTENCES,\n",
    ")\n",
    "\n",
    "# Creating the DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Checking the first batch from the train_loader\n",
    "print(type(next(iter(train_loader))[\"input_ids\"]))\n",
    "# next(iter(train_loader))[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm  # for displaying progress\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device, scheduler, loss_fn, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.float() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device, loss_fn, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.float() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)  # Send the model to GPU if available\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Scheduler for learning rate\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Training and Validation\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, device, scheduler, loss_fn, len(train_dataset)\n",
    "    )\n",
    "\n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, device, loss_fn, len(val_dataset))\n",
    "\n",
    "    print(f\"Validation loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"Colbert_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
