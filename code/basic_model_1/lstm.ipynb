{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ironic/31_9_R1GE9UI3OWCA8M.txt', 'Ironic/22_15_R3S3PYAPELGTG3.txt', 'Ironic/50_12_R24XCD6S26ID9C.txt', 'Ironic/33_7_R33YPAV3D4QU2P.txt', 'Ironic/45_4_R2S0DJ52DDQGF0.txt', 'Ironic/13_12_R37XGBTD0KEF0P.txt', 'Ironic/42_11_R1HNFW27RW2MWJ.txt', 'Ironic/51_5_R3V2K3R4BCRJ75.txt', 'Ironic/51_14_R22TEMV2FB5OD.txt', 'Ironic/29_18_R1WLZAH4TAPM55.txt']\n",
      "1254\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../../datasets/SarcasmAmazonReviewsCorpus-master\")\n",
    "dataset_path = os.path.abspath(os.curdir)\n",
    "\n",
    "data_list = list()\n",
    "data_list.extend([os.path.join(\"Ironic\", file) for file in os.listdir(os.path.join(dataset_path, \"Ironic\")) if file.endswith(\".txt\")])\n",
    "data_list.extend([os.path.join(\"Regular\", file) for file in os.listdir(os.path.join(dataset_path, \"Regular\")) if file.endswith(\".txt\")])\n",
    "\n",
    "print(data_list[:10])\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_folder_path, tokenizer):\n",
    "        self.data_folder_path = data_folder_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocabulary = list()\n",
    "\n",
    "        self.data_list = list()\n",
    "        self.data_list.extend([os.path.join(\"Ironic\", file) for file in os.listdir(os.path.join(dataset_path, \"Ironic\")) if file.endswith(\".txt\")])\n",
    "        self.data_list.extend([os.path.join(\"Regular\", file) for file in os.listdir(os.path.join(dataset_path, \"Regular\")) if file.endswith(\".txt\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in your dataset\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and process the text file at the given index and return it\n",
    "        os.chdir(self.data_folder_path)\n",
    "        file_path = self.data_list[idx]\n",
    "\n",
    "        # Get the label from the file name\n",
    "        label = 0 if 'Regular' in file_path else 1\n",
    "\n",
    "        text_data = None\n",
    "        with open(file_path, 'r', encoding=\"unicode_escape\") as file:\n",
    "            for line in file:\n",
    "                if '<REVIEW>' in line:\n",
    "                        text_data = file.readlines()\n",
    "\n",
    "        # Search for the <\\REVIEW> tag and remove it (and everything after it)\n",
    "        for i in range(len(text_data)):\n",
    "            if '</REVIEW>' in text_data[i]:\n",
    "                text_data = text_data[:i]\n",
    "                break\n",
    "\n",
    "        # Remove \\n and combine in one string\n",
    "        text_data = ' '.join([line.replace('\\n', '') for line in text_data])\n",
    "        \n",
    "        # Tokenize the text data\n",
    "        text_data = self.tokenizer(text_data)\n",
    "\n",
    "        input_ids = list()\n",
    "        for token in text_data:\n",
    "            if token not in self.vocabulary:\n",
    "                self.vocabulary.append(token)\n",
    "            input_ids.append(self.vocabulary.index(token))\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        # Return the processed data and its corresponding label (if applicable)\n",
    "        return {\n",
    "            \"text\": text_data, \n",
    "            \"input_ids\": input_ids,\n",
    "            \"label\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i', 'am', 'an', 'acquisitions', 'officer', 'for', 'an', 'artillery', 'unit', 'in', 'the', 'russian', 'army', '.', 'since', 'mafia', 'hooligans', 'stole', 'all', 'of', 'our', 'equipment', 'to', 'sell', 'to', 'kyrgyzstani', 'rebels', ',', 'we', 'have', 'been', 'looking', 'for', 'a', 'low-cost', 'alternative', 'to', 'the', 't-80', 'main', 'battle', 'tank', '.', 'after', 'successful', 'trials', 'at', 'a', 'facility', 'in', 'moscow', ',', 'this', 'so-called', 'badonkadonk', 'was', 'approved', 'for', 'use', 'in', 'the', 'chechen', 'theatre', '.', 'initial', 'reports', 'were', 'favorable', ',', 'but', 'then', 'somebody', 'noticed', 'that', 'the', 'tank', 'lacked', 'a', 'cannon', ',', 'treads', ',', 'and', 'armor', ',', 'and', 'possessed', 'the', 'engine', 'of', 'an', 'electric', 'bicycle', '.', 'it', 'did', ',', 'however', ',', 'have', 'an', 'excellent', 'audio', 'system', ',', 'but', 'this', 'failed', 'to', 'compensate', 'for', 'its', 'disappointing', '100%', 'mortality', 'rate', '.', 'recommended', 'only', 'for', 'use', 'against', 'lithuanians', '.'], 'input_ids': tensor([ 0,  1,  2,  3,  4,  5,  2,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 21, 23, 24, 25, 26, 27, 28, 29,  5, 30, 31, 32,\n",
      "        21,  9, 33, 34, 35, 36, 12, 37, 38, 39, 40, 30, 41,  8, 42, 25, 43, 44,\n",
      "        45, 46, 47,  5, 48,  8,  9, 49, 50, 12, 51, 52, 53, 54, 25, 55, 56, 57,\n",
      "        58, 59,  9, 36, 60, 30, 61, 25, 62, 25, 63, 64, 25, 63, 65,  9, 66, 18,\n",
      "         2, 67, 68, 12, 69, 70, 25, 71, 25, 27,  2, 72, 73, 74, 25, 55, 43, 75,\n",
      "        21, 76,  5, 77, 78, 79, 80, 81, 12, 82, 83,  5, 48, 84, 85, 12]), 'label': 1}\n",
      "1003 125 126\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(dataset_path, get_tokenizer('basic_english'))\n",
    "\n",
    "print(dataset[12])\n",
    "\n",
    "# Suffle the dataset\n",
    "torch.manual_seed(0)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "# Split the dataset into train, validation and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'and', 'to', 'a', 'i', 'of', \"'\", 'it']\n",
      "19036\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "tokens = []\n",
    "for data in train_dataset:\n",
    "    tokens.extend(data['text'])\n",
    "\n",
    "vocab = build_vocab_from_iterator([tokens])\n",
    "\n",
    "print(vocab.get_itos()[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_model(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(Network_model, self).__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=0.2)\n",
    "\n",
    "        # Linear\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, label_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[-1, :, :]\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, loss_fn, data_loader, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids']\n",
    "            label = data['label']\n",
    "            input_ids, label = input_ids.to(device), label.to(device)\n",
    "\n",
    "            # Get the prediction\n",
    "            outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss += loss_fn(outputs, label).item()\n",
    "\n",
    "            # Count the total number of correct predictions\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        loss /= len(data_loader)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(num_epochs, model, loss_fn, optimizer, train_loader, validation_loader, device):\n",
    "\n",
    "    evaluate(model, loss_fn, validation_loader, device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            input_ids = data['input_ids']\n",
    "            label = data['label']\n",
    "            input_ids, label = input_ids.to(device), label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad() # Clear the gradients\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(outputs, label)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model after each epoch\n",
    "        valid_loss, valid_accuracy = evaluate(model, loss_fn, validation_loader, device)\n",
    "\n",
    "        # Print Loss every epoch\n",
    "        print('Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Validation Accuracy {:.6f}'.format(epoch + 1, num_epochs, train_loss, valid_loss, valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build iterator\n",
    "train_loader = DataLoader(train_dataset)\n",
    "val_loader = DataLoader(val_dataset)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(type(data['input_ids']))\n",
    "    print(type(data['label']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on mps device\n",
      "Epoch: [1/10], Training Loss: 656.916227, Validation Loss: 0.658690, Validation Accuracy 0.712000\n",
      "Epoch: [2/10], Training Loss: 528.581546, Validation Loss: 0.790340, Validation Accuracy 0.632000\n",
      "Epoch: [3/10], Training Loss: 264.667727, Validation Loss: 1.119410, Validation Accuracy 0.608000\n",
      "Epoch: [4/10], Training Loss: 88.248260, Validation Loss: 1.555002, Validation Accuracy 0.600000\n",
      "Epoch: [5/10], Training Loss: 16.898428, Validation Loss: 1.818501, Validation Accuracy 0.688000\n",
      "Epoch: [6/10], Training Loss: 1.235500, Validation Loss: 1.986187, Validation Accuracy 0.696000\n",
      "Epoch: [7/10], Training Loss: 0.193590, Validation Loss: 2.107672, Validation Accuracy 0.696000\n",
      "Epoch: [8/10], Training Loss: 0.092499, Validation Loss: 2.230078, Validation Accuracy 0.688000\n",
      "Epoch: [9/10], Training Loss: 0.049571, Validation Loss: 2.353275, Validation Accuracy 0.688000\n",
      "Epoch: [10/10], Training Loss: 0.026950, Validation Loss: 2.477211, Validation Accuracy 0.680000\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Execution device (mps or cpu)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "model = Network_model(embedding_dim, hidden_dim, len(vocab), 2)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train(num_epochs, model, loss_fn, optimizer, train_loader, val_loader, device)\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.849635, Test Accuracy: 0.634921\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_loss, test_accuracy = evaluate(model, loss_fn, test_loader, device)\n",
    "print(\"Test Loss: {:.6f}, Test Accuracy: {:.6f}\".format(test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
