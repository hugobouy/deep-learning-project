{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis:**\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Measure the sentiment score of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence  is_sarcastic\n",
       "0           0  thirtysomething scientists unveil doomsday clo...           1.0\n",
       "1           1  dem rep. totally nails why congress is falling...           0.0\n",
       "2           2  eat your veggies: 9 deliciously different recipes           0.0\n",
       "3           3  inclement weather prevents liar from getting t...           1.0\n",
       "4           4  mother comes pretty close to using word 'strea...           1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.join(\"..\", \"datasets\", \"combined.csv\")\n",
    "dataset = pd.read_csv(dataset_path, delimiter=\",\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "thirtysomething scientists unveil doomsday clock of hair loss---- {'neg': 0.504, 'neu': 0.496, 'pos': 0.0, 'compound': -0.7269}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentence = dataset[\"sentence\"][0]\n",
    "vs = analyzer.polarity_scores(sentence)\n",
    "print(type(vs))\n",
    "print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Count some punctuation symbols and normalize the counts with the size of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dem rep. totally nails why congress is falling short on gender, racial equality\n",
      "[1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "punctuations = ['.', '!', '?', ',']\n",
    "punctuations_counts = list()\n",
    "sentence = dataset[\"sentence\"][1]\n",
    "print(sentence)\n",
    "for punctuation in punctuations:\n",
    "    punctuations_counts.append(sentence.count(punctuation))\n",
    "print(punctuations_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Count the number of noun, verb, ect. in a text and normalize it with the count of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:00:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 11.5MB/s]                    \n",
      "2023-11-29 12:00:33 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2023-11-29 12:00:33 INFO: Using device: cpu\n",
      "2023-11-29 12:00:33 INFO: Loading: tokenize\n",
      "2023-11-29 12:00:33 INFO: Loading: pos\n",
      "2023-11-29 12:00:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "constituency_parser = stanza.Pipeline(lang='en', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thirtysomething scientists unveil doomsday clock of hair loss\n",
      "thirtysomething ADJ\n",
      "scientists NOUN\n",
      "unveil VERB\n",
      "doomsday ADJ\n",
      "clock NOUN\n",
      "of ADP\n",
      "hair NOUN\n",
      "loss NOUN\n"
     ]
    }
   ],
   "source": [
    "text = dataset[\"sentence\"][0]\n",
    "doc = constituency_parser(text)\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.text)\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.upos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Create vocabulary for both sarcastic and non-sarcastic sets and measure the ratio of sarcastic and non-sarcastic words in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWords = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2), max_features=1000)\n",
    "sarcastic_sentences = dataset[dataset[\"is_sarcastic\"] == 1][\"sentence\"]\n",
    "sarcastic_bow = bagOfWords.fit_transform(sarcastic_sentences)\n",
    "vocabulary_sarcastic = bagOfWords.vocabulary_\n",
    "regular_sentences = dataset[dataset[\"is_sarcastic\"] == 0][\"sentence\"]\n",
    "regular_bow = bagOfWords.fit_transform(regular_sentences)\n",
    "vocabulary_regular = bagOfWords.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scientists': 758, 'hair': 388, 'getting': 356, 'work': 975, 'mother': 579, 'comes': 175, 'pretty': 679, 'close': 170, 'using': 921, 'word': 973, 'nearly': 593, 'failed': 296, 'government': 375, 'large': 484, 'meet': 553, 'room': 743, 'new': 597, 'area': 52, 'boy': 111, 'man': 539, 'does': 239, 'area man': 53, 'video': 925, 'game': 347, 'secret': 762, 'service': 773, 'fan': 302, 'wearing': 952, 'shirt': 777, 'day': 215, 'york': 998, 'introduces': 454, 'program': 688, 'city': 162, 'new york': 599, 'obama': 609, 'state': 829, 'speech': 818, 'law': 489, 'history': 415, 'makes': 536, 'come': 174, 'students': 842, 'report': 723, 'probably': 684, 'god': 367, 'bring': 117, 'high': 410, 'mom': 571, 'keeps': 469, 'make': 535, 'stop': 834, 'team': 867, 'having': 400, 'men': 557, 'experience': 286, 'paul': 640, 'parents': 635, 'department': 226, 'warns': 942, 'americans': 41, 'avoid': 72, 'missing': 569, 'just': 466, 'hoping': 421, 'media': 551, 'gets': 355, 'needs': 596, 'releases': 719, 'romney': 742, 'going': 369, 'door': 248, 'let': 496, 'know': 478, 'president': 677, 'dead': 217, 'stars': 825, 'long': 519, 'security': 764, 'sick': 788, 'like': 505, 'com': 173, 'massive': 543, 'reports': 724, 'modern': 570, 'american': 40, 'life': 503, 'historical': 414, 'amazing': 36, 'nation': 588, 'onion': 623, 'social': 802, 'sexual': 776, 'women': 969, 'drunk': 254, 'woman': 968, 'wishes': 966, 'husband': 435, 'majority': 534, 'people': 643, 'lives': 513, 'immediately': 445, 'visit': 928, 'study': 843, 'finds': 321, 'study finds': 844, 'trump': 903, 'hours': 426, 'dad': 210, 'hands': 391, 'phone': 650, 'local': 516, 'fear': 310, 'little': 511, 'child': 150, 'second': 761, 'business': 121, 'year': 993, 'post': 672, 'perfect': 645, 'student': 841, 'old': 621, 'family': 301, 'season': 760, 'available': 70, 'announces': 44, 'plans': 657, 'baby': 74, 'stuff': 845, 'members': 556, 'opening': 625, 'band': 78, 'walking': 936, 'crowd': 205, 'friend': 340, 'wall': 937, 'town': 897, 'announce': 43, 'change': 144, 'plan': 655, 'children': 151, 'names': 587, 'christian': 156, 'bar': 79, 'power': 675, 'love': 528, 'minutes': 567, 'figure': 318, 'supreme': 854, 'court': 198, 'talking': 863, 'supreme court': 855, 'asked': 60, 'water': 948, 'north': 605, 'feel': 311, 'non': 603, 'finally': 320, 'tells': 871, 'shut': 787, 'fuck': 342, 'meeting': 554, 'teen': 868, 'right': 738, 'unveils': 917, 'foot': 328, 'unveils new': 918, 'secretary': 763, 'party': 637, 'big': 94, 'wondering': 972, 'chinese': 152, 'congress': 184, 'wedding': 954, 'presidential': 678, 'customer': 208, 'nice': 601, 'restaurant': 733, 'things': 880, 'say': 751, 'notice': 607, 'scene': 754, 'taking': 861, 'ceo': 141, 'time': 888, 'role': 741, 'millions': 564, 'mass': 542, 'poll': 665, 'vote': 930, 'candidate': 130, 'half': 389, 'understand': 913, 'doesn': 240, 'shit': 778, 'wrote': 989, 'words': 974, 'anti': 46, 'drug': 253, 'use': 919, 'way': 949, 'tv': 910, 'character': 146, 'actually': 20, 'coffee': 171, 'feels': 313, 'thinks': 883, 'recommend': 715, 'voters': 931, 'start': 826, 'election': 263, 'story': 836, 'reason': 713, 'great': 378, 'years': 995, 'thinking': 882, 'set': 774, 'forced': 330, 'movie': 581, 'night': 602, 'lets': 497, 'died': 231, 'shooting': 781, 'complete': 180, 'idiot': 439, 'forgot': 332, 'mouth': 580, 'totally': 896, 'year old': 994, 'doing': 242, 'birth': 96, 'bus': 119, 'human': 432, 'couple': 196, 'weeks': 956, 'purchase': 693, 'tree': 898, 'making': 537, 'struggling': 839, 'company': 179, 'job': 461, 'home': 419, 'living': 514, 'public': 692, 'air': 31, 'person': 647, 'looking': 523, 'hillary': 412, 'clinton': 169, 'hillary clinton': 413, 'ahead': 30, 'fbi': 309, 'website': 953, 'facebook': 293, 'stupid': 846, 'young': 999, 'address': 22, '50': 9, 'heaven': 406, 'population': 669, 'accidentally': 15, 'cover': 199, 'space': 813, 'wants': 940, 'small': 801, 'park': 636, 'friends': 341, 'black': 99, 'days': 216, 'bear': 82, 'terrible': 873, 'girl': 359, 'wait': 933, 'boyfriend': 112, 'lot': 527, 'legal': 495, 'playing': 659, 'ball': 77, 'good': 372, 'hard': 396, 'begins': 85, 'debate': 221, 'fucking': 343, 'sense': 770, 'gay': 350, 'tell': 869, 'gop': 373, 'trying': 906, 'throw': 887, 'introduces new': 455, 'free': 338, '11': 3, 'didn': 229, 'realize': 710, 'cut': 209, 'money': 573, 'spent': 821, 'box': 110, 'face': 292, 'ex': 279, 'wife': 962, 'board': 102, 'death': 220, 'gone': 370, 'earth': 256, 'number': 608, 'planet': 656, 'ad': 21, 'ideas': 438, 'easy': 257, 'oil': 618, 'price': 681, 'fast': 305, 'pope': 667, 'john': 463, 'owner': 630, 'school': 755, 'sound': 810, 'experts': 287, 'past': 639, 'boring': 106, 'boss': 108, 'came': 127, 'today': 891, 'guy': 386, 'boys': 113, 'white': 960, 'house': 427, 'white house': 961, 'son': 805, 'review': 736, 've': 922, 'away': 73, 'chance': 143, 'got': 374, 'll': 515, 'continue': 190, 'takes': 860, 'run': 744, 'clearly': 168, 'buy': 122, 'hope': 420, 'eyes': 291, 'teacher': 866, 'week': 955, 'campaign': 128, 'intelligence': 451, 'place': 654, 'claim': 163, 'writing': 986, 'entire': 273, 'inside': 448, 'real': 708, 'humans': 433, 'new study': 598, 'million': 563, 'bag': 76, 'chicken': 149, 'car': 131, 'soon': 807, 'drive': 251, 'quick': 698, 'proud': 690, 'cool': 192, 'album': 33, 'glass': 366, 'live': 512, 'apparently': 49, 'starting': 828, 'type': 911, 'looks': 524, 'college': 172, 'self': 767, 'better': 91, 'sign': 789, 'response': 730, 'die': 230, '15': 5, 'plot': 660, 'months': 575, 'america': 39, 'ends': 269, 'store': 835, 'early': 255, 'attempt': 66, 'month': 574, 'fat': 306, 'ice': 436, 'concerned': 183, 'following': 326, 'straight': 837, 'article': 58, 'george': 354, 'admits': 26, 'king': 476, 'future': 346, 'society': 803, 'citizens': 161, 'moment': 572, 'line': 507, 'average': 71, 'think': 881, 'stay': 832, 'prison': 682, 'turns': 909, 'discover': 237, 'jobs': 462, 'meant': 550, 'eating': 259, 'race': 703, 'minute': 566, 'missed': 568, 'obviously': 611, 'food': 327, '000': 0, 'end': 268, 'eye': 290, 'brain': 114, 'extra': 289, 'moon': 576, 'act': 17, 'says': 753, 'sitting': 796, 'really': 712, 'help': 408, 'hot': 424, 'bush': 120, 'spend': 819, '30': 7, 'dance': 212, 'father': 307, 'sleep': 800, 'watching': 947, 'reveals': 735, 'care': 133, 'left': 494, 'cat': 136, 'coworker': 200, 'ways': 950, 'win': 963, 'lose': 525, 'ability': 10, 'dies': 232, 'grandma': 377, 'christmas': 159, 'biden': 93, 'ryan': 746, 'check': 148, 'area woman': 54, 'used': 920, 'paper': 634, 'energy': 270, 'heart': 405, 'body': 103, 'open': 624, 'sex': 775, 'leaves': 493, 'note': 606, 'lack': 482, 'opinion': 626, 'late': 485, 'source': 812, 'billion': 95, 'saying': 752, '10': 1, 'senate': 769, 'bathroom': 81, 'pro': 683, 'percent': 644, 'putting': 694, 'head': 401, 'look': 521, 'saw': 750, 'series': 771, 'thing': 879, 'term': 872, '12': 4, 'rock': 740, 'murder': 585, 'defense': 224, 'reading': 706, 'book': 104, 'excited': 282, 'heard': 404, 'hand': 390, 'kids': 471, 'kind': 475, 'country': 195, 'kid': 470, 'goes': 368, 'prove': 691, 'sort': 809, 'point': 661, 'single': 794, 'hear': 403, 'given': 362, 'huge': 430, 'natural': 590, 'cause': 138, 'group': 380, 'called': 124, 'audience': 68, 'gives': 363, 'standing': 823, 'list': 509, 'facts': 295, 'exactly': 280, 'administration': 24, 'version': 924, 'watch': 946, 'hate': 398, 'posts': 674, 'interesting': 452, 'points': 662, 'coming': 176, 'true': 901, 'telling': 870, 'truth': 904, 'thousands': 885, 'apple': 50, 'unable': 912, 'decision': 222, 'far': 304, 'washington': 943, 'killed': 473, 'lies': 502, 'completely': 181, 'national': 589, 'stick': 833, 'horrible': 422, 'admit': 25, 'worth': 982, '20': 6, 'considering': 188, 'daughter': 214, 'grow': 381, 'felt': 314, 'middle': 561, 'mr': 583, 'ready': 707, 'case': 135, 'happens': 394, 'star': 824, 'screen': 759, 'music': 586, 'bad': 75, 'idea': 437, 'constitution': 189, 'responsible': 731, 'feeling': 312, 'ground': 379, 'wrong': 988, 'matter': 544, 'claims': 164, 'attack': 65, 'sure': 856, 'trip': 900, 'fetus': 316, 'gun': 383, 'short': 783, 'driving': 252, 'clear': 167, 'thought': 884, 'world': 978, 'evidence': 276, 'age': 27, 'insult': 450, 'asks': 62, 'sun': 849, 'surprised': 857, 'science': 756, 'stuck': 840, 'machine': 532, 'francis': 337, 'jesus': 460, 'disappointed': 236, 'married': 541, 'adds': 23, 'hit': 416, 'wouldn': 983, 'center': 140, 'size': 798, 'waiting': 934, 'thanks': 876, 'united': 914, 'remember': 722, 'eat': 258, 'save': 749, 'control': 191, 'christ': 155, 'lost': 526, 'health': 402, 'cancer': 129, 'dinner': 235, 'piece': 653, 'tax': 864, 'create': 202, '40': 8, 'high school': 411, 'office': 613, 'running': 745, 'horse': 423, 'pay': 641, 'morning': 578, 'born': 107, 'fans': 303, 'works': 977, 'girlfriend': 360, 'shop': 782, 'song': 806, 'hell': 407, 'iraq': 456, 'judge': 465, 'believe': 87, 'sad': 747, 'animals': 42, 'calls': 126, 'member': 555, 'community': 178, 'mental': 558, 'ass': 63, 'dog': 241, 'floor': 324, 'best': 89, 'page': 631, 'likely': 506, 'war': 941, 'violence': 927, 'red': 717, 'political': 664, 'speaking': 815, 'employees': 267, 'desk': 227, 'loved': 529, 'ones': 622, 'wear': 951, 'quickly': 699, 'internet': 453, 'want': 938, 'working': 976, 'respect': 728, 'issues': 459, 'known': 480, 'knowledge': 479, 'career': 134, 'knows': 481, 'universe': 915, 'realizes': 711, 'link': 508, 'absolutely': 13, 'seeing': 765, 'hasn': 397, 'stand': 822, 'turned': 908, 'amendment': 38, 'police': 663, 'happened': 393, 'happy': 395, 'try': 905, 'order': 627, 'yes': 997, 'did': 228, 'vows': 932, 'religious': 721, 'original': 628, 'play': 658, 'instead': 449, 'class': 165, 'mention': 559, 'questions': 697, 'special': 816, 'games': 348, 'fox': 336, 'light': 504, 'able': 11, 'mueller': 584, 'rich': 737, 'picture': 652, 'wing': 964, 'level': 498, 'ago': 28, 'taken': 859, 'film': 319, 'offers': 612, 'based': 80, 'latest': 487, 'form': 333, 'super': 850, 'turn': 907, 'blood': 100, 'table': 858, 'hour': 425, 'total': 895, 'won': 970, 'apartment': 48, 'gift': 358, 'culture': 206, 'deep': 223, 'spends': 820, 'shot': 784, 'republican': 725, 'employee': 266, 'don': 243, 'bed': 84, 'dream': 250, 'situation': 797, 'truly': 902, 'freedom': 339, 'information': 447, 'relationship': 718, 'talk': 862, 'maybe': 545, 'female': 315, 'hundreds': 434, 'times': 889, 'wolf': 967, 'grade': 376, 'sorry': 808, 'effort': 262, 'prevent': 680, 'explain': 288, 'street': 838, 'told': 892, 'product': 687, 'weird': 958, 'issue': 458, 'guys': 387, 'seen': 766, 'shows': 786, 'giant': 357, 'ask': 59, 'low': 530, 'ignore': 442, 'worse': 980, 'test': 874, 'news': 600, 'popular': 668, 'record': 716, 'kill': 472, 'question': 696, 'pick': 651, 'according': 16, 'mean': 547, 'rights': 739, 'voice': 929, 'actual': 19, 'conservative': 185, 'speak': 814, 'military': 562, 'giving': 364, 'joke': 464, 'holding': 418, 'michael': 560, 'mind': 565, 'learn': 491, 'breaking': 116, 'pages': 632, 'read': 705, 'tried': 899, 'abortion': 12, 'return': 734, 'enjoy': 272, 'forget': 331, 'shouldn': 785, 'male': 538, 'computer': 182, 'wanted': 939, 'worried': 979, 'leave': 492, 'written': 987, 'example': 281, 'clean': 166, 'theory': 878, 'wasn': 944, 'crime': 204, 'weight': 957, 'normal': 604, 'pregnant': 676, 'favorite': 308, 'accept': 14, 'wonder': 971, 'allow': 34, 'imagine': 444, 'fine': 322, 'beliefs': 86, 'support': 851, 'date': 213, '100': 2, 'general': 353, 'justice': 468, 'fact': 294, 'went': 959, 'buying': 123, 'simple': 792, 'author': 69, 'wow': 984, 'english': 271, 'moving': 582, 'bit': 98, 'rest': 732, 'need': 594, 'marriage': 540, 'fall': 299, 'outside': 629, 'poor': 666, 'knew': 477, 'glad': 365, 'al': 32, 'action': 18, 'aren': 55, 'wish': 965, 'species': 817, 'fun': 344, 'problems': 686, 'mccain': 546, 'eric': 274, 'meaning': 548, 'unless': 916, 'longer': 520, 'reality': 709, 'hold': 417, 'okay': 620, 'land': 483, 'killing': 474, 'personal': 648, 'supposed': 853, 'site': 795, 'walk': 935, 'cd': 139, 'view': 926, 'blue': 101, 'excuse': 283, 'happen': 392, 'funny': 345, 'states': 831, 'important': 446, 'card': 132, 'lying': 531, 'shoot': 780, 'tired': 890, 'medical': 552, 'forward': 335, 'asking': 61, 'doctor': 238, 'crazy': 201, 'education': 260, 'changed': 145, 'lie': 501, 'position': 670, 'fight': 317, 'listen': 510, 'ignorance': 440, 'sounds': 811, 'subject': 847, 'took': 893, 'catholic': 137, 'church': 160, 'don know': 244, 'amazon': 37, 'choice': 153, 'needed': 595, 'oh': 614, 'yeah': 992, 'seriously': 772, 'difference': 233, 'liberals': 500, 'beautiful': 83, 'created': 203, 'break': 115, 'guns': 385, 'research': 727, 'evil': 277, 'deal': 218, 'different': 234, 'racist': 704, 'fair': 297, 'republicans': 726, 'said': 748, 'years ago': 996, 'girls': 361, 'shocked': 779, 'skin': 799, 'allowed': 35, 'problem': 685, 'quality': 695, 'waste': 945, 'near': 592, 'conservatives': 186, 'bible': 92, 'simply': 793, 'force': 329, 'bought': 109, 'means': 549, 'answer': 45, 'course': 197, 'sell': 768, 'just like': 467, 'laws': 490, 'pants': 633, 'fit': 323, 'moral': 577, 'look like': 522, 'bunch': 118, 'consider': 187, 'liberal': 499, 'correct': 193, 'perfectly': 646, 'worst': 981, 'nature': 591, 'religion': 720, 'later': 486, 'current': 207, 'write': 985, 'common': 177, 'suddenly': 848, 'started': 827, 'birthday': 97, 'calling': 125, 'teach': 865, 'arguments': 57, 'quite': 700, 'pass': 638, 'gay marriage': 351, 'argument': 56, 'haven': 399, 'books': 105, 'archie': 51, 'laugh': 488, 'assume': 64, 'evolution': 278, 'respond': 729, 'major': 533, 'pen': 642, 'follow': 325, 'agree': 29, 'characters': 147, 'gave': 349, 'christianity': 157, 'dear': 219, 'illegal': 443, 'thank': 875, 'edward': 261, 'isn': 457, 'somebody': 804, 'personally': 649, 'attention': 67, 'couldn': 194, 'anymore': 47, 'statement': 830, 'possible': 671, 'obvious': 610, 'especially': 275, 'gun control': 384, 'scientific': 757, 'damn': 211, 'false': 300, 'oh wait': 615, 'exist': 284, 'quote': 702, 'reasons': 714, 've got': 923, 'huh': 431, 'thread': 886, 'bet': 90, 'gays': 352, 'choose': 154, 'definition': 225, 'don think': 245, 'hey': 409, 'gonna': 371, 'ya': 991, 'proof': 689, 'forum': 334, 'silly': 790, 'faith': 298, 'expect': 285, 'topic': 894, 'don want': 246, 'doubt': 249, 'posted': 673, 'christians': 158, 'guess': 382, 'oh yeah': 616, 'certainly': 142, 'ok': 619, 'oh yes': 617, 'ignorant': 441, 'suppose': 852, 'logic': 517, 'emoticonxgood': 264, 'thats': 877, 'emoticonxrolleyes': 265, 'lol': 518, 'http': 428, 'www': 990, 'http www': 429, 'dont': 247, 'simone': 791, 'quot': 701, 'bella': 88}\n",
      "{'totally': 901, 'congress': 174, 'short': 793, 'eat': 268, 'different': 239, 'white': 966, 'ways': 957, 'lots': 526, 'parents': 631, 'know': 477, 'considered': 177, 'father': 317, 'amazing': 44, 'told': 898, 'daughter': 215, 'sex': 789, 'special': 825, 'current': 206, 'chris': 143, 'hillary': 411, 'clinton': 159, 'hillary clinton': 412, 'trump': 908, 'leave': 493, 'person': 641, 'like': 504, 'phone': 644, 'away': 72, 'killed': 471, 'press': 684, 'live': 511, 'won': 974, 'look': 519, 'republicans': 738, 'obamacare': 610, 'new': 596, 'things': 886, 'learned': 492, 'month': 572, 'having': 398, 'baby': 74, 'points': 663, 'single': 802, 'high': 407, 'school': 769, 'game': 350, 'high school': 408, 'took': 899, 'years': 995, 'come': 164, 'say': 766, '10': 1, 'life': 502, 'lessons': 496, 'older': 617, 'young': 999, 'heart': 404, 'stars': 834, 'hair': 387, 'prove': 698, 'getting': 361, 'isn': 453, 'bad': 75, 'police': 664, 'woman': 972, 'story': 845, 'attack': 67, 'men': 553, 'update': 924, 'best': 87, 'man': 538, 'child': 138, 'turned': 913, 'gay': 353, 'couple': 190, 'wants': 948, 'supreme': 860, 'court': 193, 'hear': 402, 'case': 124, 'supreme court': 861, 'dead': 218, 'google': 373, 'people': 637, 'think': 887, 'texas': 880, 'style': 853, 'hot': 420, 'marriage': 542, 'christians': 145, 'speak': 824, 'love': 527, 'hate': 396, 'cut': 207, 'face': 302, 'gop': 374, 'vote': 941, 'president': 682, 'donald': 255, 'makes': 536, 'sick': 797, 'donald trump': 256, 'message': 556, 'facebook': 303, 'talking': 869, 'season': 776, 'meet': 551, 'year': 993, 'old': 616, 'year old': 994, 'gun': 382, 'control': 181, 'lost': 524, 'violence': 939, 'gun control': 383, 'kids': 469, 'loved': 528, 'want': 946, 'make': 535, 'start': 835, 'pick': 648, 'wins': 970, 'democratic': 229, 'argument': 57, 'cost': 186, 'china': 140, 'north': 602, 'korea': 480, 'sanders': 763, 'watching': 954, 'mind': 563, 'film': 330, 'super': 857, 'political': 666, 'seen': 780, 'obama': 609, 'just': 462, 'student': 849, 'election': 272, 'york': 998, 'city': 149, 'new york': 597, 'biggest': 91, 'administration': 34, 'number': 607, 'students': 850, 'america': 47, 'way': 956, 'speed': 829, 'study': 851, 'finds': 333, 'american': 48, 'poor': 668, 'report': 736, 'worst': 983, 'playing': 658, 'built': 107, 'states': 839, 'help': 406, 'million': 561, 'lead': 488, 'cover': 194, 'watch': 953, '11': 3, '000': 0, 'video': 936, 'cop': 183, 'girl': 362, 'wedding': 961, 'day': 216, 'leader': 489, 'anti': 52, 'abortion': 18, 'gets': 360, 'win': 969, 'wall': 945, 'said': 762, 'run': 757, 'cruz': 204, 'books': 98, 'surprised': 863, 'll': 514, 'democrats': 230, 'party': 633, 'tv': 914, 'ad': 31, 'perfectly': 639, 'album': 40, 'kim': 473, 'provide': 699, 'true': 906, 'stop': 842, 'home': 417, 'record': 727, 'happens': 393, 'data': 213, 'campaign': 118, 'big': 90, 'house': 422, 'law': 486, 'fall': 308, 'movies': 579, 'mom': 569, 'rule': 755, 'says': 768, 'order': 623, 'director': 243, 'james': 457, 'making': 537, 'movie': 578, 'truth': 909, '40': 14, 'really': 723, 'does': 246, 'problems': 693, 'race': 712, 'line': 506, 'sexual': 790, 'search': 775, 'perfect': 638, 'space': 823, 'ideas': 431, 'small': 808, 'easily': 266, 'americans': 49, 'risk': 751, 'iraq': 452, 'let': 497, 'state': 837, 'fight': 328, 'shouldn': 795, 'judge': 461, 'apple': 53, 'iphone': 449, 'lets': 498, 'mike': 559, 'point': 662, 'machine': 531, 'netflix': 595, 'thing': 885, 'doing': 250, 'called': 114, 'close': 160, 'sad': 760, 'self': 782, 'red': 728, 'head': 399, 'late': 484, 'work': 977, 'ready': 719, 'force': 339, 'personal': 642, 'memory': 552, 'remember': 735, 'community': 168, 'republican': 737, 'card': 121, 'aren': 55, 'try': 910, 'listen': 509, 'product': 695, 'evolution': 284, 'text': 881, 'rock': 753, 'need': 592, 'change': 130, 'justice': 464, 'politics': 667, 'culture': 205, 'wars': 950, 'longer': 518, 'support': 858, 'israel': 454, 'senate': 783, 'huge': 426, 'return': 744, 'stage': 830, 'important': 434, 'reasons': 725, 'right': 748, 'songs': 814, 'looks': 521, 'exactly': 287, '2015': 11, 'taking': 867, 'public': 700, 'early': 263, 'travel': 904, 'ban': 76, 'world': 981, 'learn': 491, 'worth': 984, 'weeks': 963, 'thanks': 883, 'happen': 391, 'internet': 447, 'social': 809, 'media': 550, 'maybe': 547, 'women': 973, 'real': 720, 'hope': 419, 'ex': 286, 'children': 139, 'families': 310, 'fit': 335, 'vs': 943, 'talk': 868, 'action': 28, 'south': 822, 'death': 221, 'bush': 108, 'calls': 115, 'plan': 653, 'link': 507, 'changed': 131, 'color': 162, 'news': 598, 'don': 251, 'feel': 323, 'white house': 967, 'bernie': 85, 'played': 655, 'com': 163, 'bernie sanders': 86, 'victims': 935, 'accused': 26, 'john': 460, 'lack': 481, 'friend': 344, 'running': 758, 'views': 938, 'open': 620, 'tells': 876, 'powerful': 678, 'stories': 844, 'break': 104, 'dark': 212, 'series': 785, 'claims': 152, 'talks': 870, 'history': 413, 'family': 311, 'price': 687, 'prison': 688, 'used': 927, 'second': 777, 'conservative': 175, 'team': 872, 'die': 236, 'create': 195, 'page': 628, 'shooting': 792, 'married': 543, 'guy': 385, 'pretty': 685, 'version': 934, 'drug': 261, 'muslim': 584, 'common': 167, 'daily': 209, 'road': 752, 'christian': 144, 'teen': 874, 'statement': 838, 'black': 94, 'lives': 512, 'wife': 968, 'posts': 675, 'photo': 645, 'questions': 706, 'issues': 456, 'continue': 180, 'king': 475, 'chance': 129, 'words': 976, 'guys': 386, 'impossible': 435, 'list': 508, 'march': 539, 'company': 169, 'secret': 778, 'society': 810, 'population': 670, 'matter': 546, 'bible': 89, 'book': 97, 'player': 656, 'actually': 30, 'pope': 669, 'got': 375, 'chief': 137, 'tweets': 915, 'claim': 151, 'reveals': 745, 'national': 586, 'dangerous': 211, 'middle': 558, 'supposed': 859, 'crime': 201, 'half': 388, '20': 10, 'dad': 208, 'time': 894, 'involved': 448, 'weapons': 958, 'takes': 866, 'long': 517, 'end': 275, 'age': 36, 'crisis': 203, 'save': 764, 'knew': 476, 'wanted': 947, 'probably': 691, 'uses': 928, 'pay': 636, 'russia': 759, 'defense': 226, 'clear': 155, 'large': 483, '30': 13, 'eating': 269, 'ask': 61, 'straight': 846, 'bring': 105, 'numbers': 608, 'schools': 770, 'problem': 692, 'dog': 249, 'days': 217, 'son': 812, 'deal': 219, 'simple': 800, 'light': 503, 'equal': 279, 'admit': 35, 'following': 337, 'god': 368, 'reading': 718, 'sorry': 817, 'fans': 313, 'free': 342, 'girls': 363, 'beautiful': 81, 'screen': 774, 'thinking': 888, 'use': 926, 'assault': 64, 'today': 897, 'didn': 235, 'thousands': 891, 'hand': 389, 'skin': 806, 'care': 122, 'good': 372, 'cause': 126, 'war': 949, 'couldn': 187, 'stuff': 852, 'star': 833, '50': 15, 'night': 600, 'favorite': 318, 'country': 189, 'trying': 911, 'female': 326, 'faith': 307, 'times': 895, 'digital': 241, 'gave': 352, 'speech': 828, 'tips': 896, 'wrong': 990, 'car': 120, 'lot': 525, 'better': 88, 'post': 674, 'rape': 713, 'extra': 299, 'coming': 166, 'entire': 278, 'came': 116, 'market': 541, '13': 5, 'likely': 505, 'genetic': 358, 'amazon': 45, 'fast': 316, 'food': 338, 'california': 113, 'answer': 51, 'sleep': 807, 'boy': 102, 'final': 331, 'hold': 415, 'presidential': 683, 'money': 571, 'fighting': 329, 'word': 975, 'education': 270, 'college': 161, '12': 4, 'abuse': 21, 'holiday': 416, 'choose': 142, 'shows': 796, 'creationist': 199, 'shot': 794, 'level': 500, 'science': 771, 'question': 705, 'wasn': 952, 'rise': 750, 'fear': 319, 'photos': 646, 'cops': 184, 'looking': 520, 'rules': 756, 'thought': 890, 'health': 400, 'rest': 741, 'easy': 267, 'health care': 401, 'stay': 840, 'song': 813, 'michael': 557, 'office': 613, 'web': 959, 'design': 231, 'twitter': 916, 'place': 652, 'mental': 554, 'week': 962, 'image': 433, 'changes': 132, 'completely': 171, 'arguments': 58, 'rights': 749, 'tell': 875, 'ok': 615, 'basic': 78, 'taken': 865, 'players': 657, 'little': 510, 'worked': 978, 'emotional': 274, 'needs': 594, 'guide': 381, 'moment': 570, 'funny': 348, 'suspect': 864, 'attacks': 68, 'scientists': 773, 'near': 590, 'body': 96, 'address': 33, 'turn': 912, 'available': 71, '14': 6, 'finally': 332, 'months': 573, 'mother': 576, 'power': 677, 'future': 349, 'kind': 474, 'keyboard': 467, 'far': 315, 'picture': 649, 'leaders': 490, 'went': 965, 'paul': 635, 'original': 625, 'cable': 112, 'major': 533, 'loss': 523, 'human': 427, 'cases': 125, 'giving': 366, 'debate': 222, 'cool': 182, 'tax': 871, 'review': 746, 'quickly': 708, 'church': 147, 'threat': 893, 'worse': 982, '100': 2, 'write': 987, 'letter': 499, 'dance': 210, 'idea': 430, 'lower': 530, 'clean': 154, 'government': 376, 'released': 732, 'living': 513, 'guns': 384, 'yes': 997, 'working': 979, 'gives': 365, 'birth': 92, 'nation': 585, 'later': 485, 'christmas': 146, 'buy': 110, 'according': 24, 'gps': 377, 'goes': 369, 'release': 731, 'ago': 37, 'years ago': 996, 'going': 370, 'despite': 232, 'majority': 534, 'citizens': 148, 'music': 583, 'iran': 451, 'nuclear': 606, 'device': 233, 'hard': 395, 'event': 281, 'low': 529, 'key': 466, 'doesn': 248, 'millions': 562, 'voters': 942, 'course': 192, 'job': 459, 'explains': 298, 'term': 877, 'business': 109, 've': 932, 've seen': 933, 'sense': 784, 'difference': 238, 'dear': 220, 'cancer': 119, 'relationship': 730, 'reason': 724, 'wish': 971, 'lose': 522, 'rate': 714, 'non': 601, 'freedom': 343, 'comes': 165, 'air': 39, 'hit': 414, 'thinks': 889, 'proof': 696, 'definitely': 227, 'asking': 63, 'killing': 472, 'success': 855, 'huffpost': 425, 'using': 929, 'form': 341, 'saying': 767, 'gone': 371, 'art': 59, 'games': 351, 'realize': 722, 'set': 788, '2016': 12, 'area': 54, 'tried': 905, 'push': 703, 'past': 634, 'explain': 297, 'play': 654, '16': 8, 'performance': 640, 'reality': 721, 'missing': 566, 'mark': 540, 'fun': 347, 'minute': 564, 'washington': 951, 'step': 841, 'inside': 442, 'response': 740, 'bought': 100, 'global': 367, 'model': 567, 'summer': 856, 'religious': 734, 'morning': 575, 'email': 273, 'united': 921, 'generation': 357, 'fair': 306, 'instead': 443, 'religion': 733, 'moral': 574, 'did': 234, 'private': 689, 'security': 779, 'safe': 761, 'climate': 157, 'climate change': 158, 'mean': 548, 'heard': 403, 'movement': 577, 'audio': 69, 'voice': 940, 'natural': 587, 'camera': 117, 'water': 955, 'check': 136, 'seriously': 786, 'kill': 470, 'soon': 816, 'protect': 697, 'pain': 629, 'happy': 394, 'topic': 900, 'means': 549, 'created': 196, 'honest': 418, 'unit': 920, 'quote': 711, 'false': 309, 'quite': 709, 'stand': 831, 'don know': 252, 'technology': 873, 'abortions': 19, 'opinion': 621, 'act': 27, 'driving': 260, 'feature': 320, 'expect': 294, 'fan': 312, 'george': 359, 'pregnant': 680, 'policy': 665, 'sounds': 820, 'add': 32, 'federal': 322, 'read': 716, 'reviews': 747, 'murder': 582, 'civil': 150, 'group': 379, 'understand': 919, 'follow': 336, 'military': 560, 'concept': 173, 'wouldn': 985, 'box': 101, 'feels': 324, 'known': 478, 'access': 23, 'legal': 495, 'wow': 986, 'quality': 704, 'just like': 463, 'share': 791, 'pro': 690, 'actual': 29, 'quick': 707, 'fact': 304, '15': 7, 'track': 903, 'possible': 672, 'exist': 291, 'include': 436, 'amendment': 46, 'uk': 918, 'size': 805, 'universe': 922, 'street': 847, 'main': 532, 'strong': 848, 'born': 99, 'fine': 334, 'online': 619, 'easier': 265, 'mr': 581, 'happened': 392, 'date': 214, 'great': 378, 'weight': 964, 'knows': 479, 'site': 803, 'mass': 544, 'touch': 902, 'writing': 988, 'considering': 178, 'sign': 798, 'energy': 276, 'awesome': 73, 'jesus': 458, 'account': 25, 'decide': 223, 'general': 356, 'earth': 264, 'result': 742, 'illegal': 432, 'modern': 568, 'evidence': 282, 'kid': 468, 'issue': 455, 'experience': 296, 'truly': 907, 'moving': 580, 'left': 494, 'hours': 421, 'laws': 487, 'decided': 224, 'ray': 715, 'gay marriage': 354, '18': 9, 'highly': 410, 'room': 754, 'board': 95, 'class': 153, 'higher': 409, 'characters': 134, 'minutes': 565, 'physical': 647, 'evil': 283, 'author': 70, 'doubt': 258, 'facts': 305, 'criminals': 202, 'test': 879, 'outside': 626, 'plus': 661, 'regular': 729, 'friends': 345, 'species': 826, 'animals': 50, 'especially': 280, 'theory': 884, 'features': 321, 'felt': 325, 'pregnancy': 679, 'included': 437, 'oh': 614, 'note': 603, 'store': 843, 'value': 931, 'potential': 676, 'service': 787, 'carry': 123, 'research': 739, 'bit': 93, 'paper': 630, 'allow': 41, 'written': 989, 'keeps': 465, 'plot': 660, 'fully': 346, 'sort': 818, 'results': 743, 'belief': 82, 'sure': 862, 'beliefs': 83, 'directly': 242, 'forced': 340, 'expected': 295, 'buying': 111, 'haven': 397, 'saw': 765, 'character': 133, 'scientific': 772, 'process': 694, 'drive': 259, 'humans': 428, 'based': 77, 'eyes': 301, 'information': 441, 'view': 937, 'hands': 390, 'source': 821, 'works': 980, 'pictures': 650, 'needed': 593, 'present': 681, 'consider': 176, 'computer': 172, 'constitution': 179, 'choice': 141, 'option': 622, 'hell': 405, 'gays': 355, 'website': 960, 'lie': 501, 'brain': 103, 'creationism': 198, 'countries': 188, 'agree': 38, 'creation': 197, 'couples': 191, 'enjoy': 277, 'reader': 717, 'sound': 819, 'charge': 135, 'decision': 225, 'novel': 605, 'died': 237, 'subject': 854, 'individual': 440, 'increase': 439, 'effect': 271, 'id': 429, 'sony': 815, 'type': 917, 'nice': 599, 'absolutely': 20, 'started': 836, 'simply': 801, 'fantastic': 314, 'asked': 62, 'thank': 882, 'believe': 84, 'wait': 944, 'certainly': 128, 'definition': 228, 'allowed': 42, 'plenty': 659, 'mention': 555, 'don think': 253, 'difficult': 240, 'compared': 170, 'accept': 22, 'including': 438, 'allows': 43, 'interested': 445, 'don want': 254, 'possibly': 673, 'terms': 878, 'excellent': 290, 'article': 60, 'org': 624, 'guess': 380, 'existence': 292, 'nature': 589, 'interesting': 446, 'able': 17, 'intelligent': 444, 'material': 545, 'brought': 106, 'thread': 892, 'atheist': 66, 'purchase': 701, 'position': 671, 'obvious': 611, 'piece': 651, 'laptop': 482, 'extremely': 300, 'necessary': 591, 'exists': 293, 'example': 288, 'standard': 832, 'dna': 245, 'argue': 56, 'particular': 632, 'selection': 781, 'given': 364, 'disagree': 244, 'notice': 604, 'ones': 618, 'yeah': 992, 'situation': 804, 'fetus': 327, 'unless': 923, 'dont': 257, 'ability': 16, 'similar': 799, 'dvd': 262, 'does mean': 247, 'assume': 65, 'specific': 827, 'battery': 80, 'purchased': 702, 'obviously': 612, 'usually': 930, 'http': 423, 'clearly': 156, 'logical': 516, 'personally': 643, 'creationists': 200, 'logic': 515, 'www': 991, 'http www': 424, 'correct': 185, 'certain': 127, 'natural selection': 588, 'basis': 79, 'evolutionary': 285, 'examples': 289, 'previous': 686, 'recommend': 726, 'overall': 627, 'software': 811, 'quot': 710, 'usb': 925, 'ipod': 450}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_sarcastic)\n",
    "print(vocabulary_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: measure the average similarity between the title and keywords in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_title = \"Fresh Whole Rabbit (Misc.)\"\n",
    "product_title2 = \"If Democrats Had Any Brains, They'd Be Republicans (Hardcover)\"\n",
    "review = \"I'll keep this short and sweet.  We ordered one of these rabbits for our children this Easter and boy what a surprise.  It is NOT a living rabbit.  Someone has killed this rabbit and skinned it, I suppose for eating.  Anyway, our children were traumatized and Easter is not the same holiday that it used to be for us.  On the upside, we don't have to fill their Easter baskets anymore as we told them the Easter bunny was killed by Amazon. P.S.  The rabbit tasted very good.\"\n",
    "review2 = \"I did buy this book. Liberals will not buy this book, so their opinion is worthless. Besides most of them cant even read clearly enough to get the joke. Anger and frustration that Conservatives are clever and witty with a dash of smarta**, and they resort to reviewing a book that we know they wouldnt spend a penny on. Besides it would take too much of their welfare check to buy it! Quit trying to sabotage this book. Just go away.\"\n",
    "review2_b = \"Coulter writes at what she does best, finding witty or witty sounding excuses to support Republicans over Democrats in any circumstance.  Frankly, about the only thing she and Democrats probably agree on is that Bush Jr. did not do a good job with the war in Iraq, Afghanistan, and 9/11.  But of course, Coulter has some kind of excuse to blame Bush's responsibility on 'any old Democrat' even though Democrats may have nothing to do with Bush Jr.'s personal responsibilities or his decision for the country. I think she makes some good points, and I think some things are just put out there and that she doesn't listen. She just wants only her opinion to be right to an extreme.  She's good at analyzing situations, but she would not be good for a government position requiring much trust to keep stability, that is for sure.  On the other hand, you probably want her to be your Republican lobbyist. A 'friend' a 'Coulter Jr.' told me about how great this book is.  He acts just like Coulter, but just doesn't publish books and goes out and speaks like she does.  Otherwise, he would probably be doing at least okay- (Coulter created and kept her niche first.)  I am not particularly Democrat or Republican, but I try to give everything a chance.  This book, while giving some fresh perspectives I would not have thought of, is quite hit or miss, too opinionated, and not always reasoning things out enough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82208162391359"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = nlp(\"dog\")\n",
    "token2 = nlp(\"cat\")\n",
    "token1.similarity(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government position requiring much trust', 'even though democrats may', \"coulter jr .' told\", 'niche first .)', \"bush jr .'\", 'witty sounding excuses', 'democrats probably agree', 'always reasoning things', 'bush jr', 'finding witty', 'probably want', 'blame bush', 'like coulter', 'coulter writes', 'coulter created', 'support republicans', 'speaks like', 'quite hit', 'publish books', 'personal responsibilities', 'particularly democrat', 'old democrat', 'least okay', 'keep stability', 'give everything', 'fresh perspectives', 'analyzing situations', 'would probably', 'republican lobbyist', 'good points', 'good job', 'democrats', 'coulter', 'things', 'republican', 'good', 'good', 'would', 'would', 'war', 'wants', 'try', 'thought', 'think', 'think', 'thing', 'sure', 'right', 'responsibility', 'put', 'otherwise', 'opinionated', 'opinion', 'nothing', 'miss', 'makes', 'listen', 'kind', 'kept', 'iraq', 'hand', 'great', 'goes', 'giving', 'friend', 'frankly', 'extreme', 'excuse', 'enough', 'decision', 'course', 'country', 'circumstance', 'chance', 'book', 'book', 'best', 'afghanistan', 'acts', '9', '11']\n",
      "['republicans', 'hardcover', 'democrats', 'brains']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "rake_nltk_var = Rake()\n",
    "rake_nltk_var.extract_keywords_from_text(review2_b)\n",
    "keyword_extracted_review = rake_nltk_var.get_ranked_phrases()\n",
    "rake_nltk_var.extract_keywords_from_text(product_title2)\n",
    "keyword_extracted_title = rake_nltk_var.get_ranked_phrases()\n",
    "print(keyword_extracted_review)\n",
    "print(keyword_extracted_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_from_keyword_extracted(keyword_extracted):\n",
    "    vectorizer = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer())\n",
    "    matrix = vectorizer.fit_transform(keyword_extracted)\n",
    "    counts_dict = dict()\n",
    "    i = 0\n",
    "    for word in vectorizer.get_feature_names_out():\n",
    "        counts_dict[word] = matrix.sum(axis=0).tolist()[0][i]\n",
    "        i += 1\n",
    "    counts_dict = dict(sorted(counts_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    # Remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        if punctuation in counts_dict.keys():\n",
    "            counts_dict.pop(punctuation)\n",
    "    return counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugobouy/miniconda3/envs/dl_project/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coulter': 5, 'democrat': 5, 'good': 4, 'book': 3, 'bush': 3, 'jr': 3, 'probably': 3, 'republican': 3, 'thing': 3, 'would': 3, 'excuse': 2, 'like': 2, 'responsibility': 2, 'think': 2, 'want': 2, 'witty': 2, '11': 1, '9': 1, 'act': 1, 'afghanistan': 1, 'agree': 1, 'always': 1, 'analyzing': 1, 'best': 1, 'blame': 1, 'chance': 1, 'circumstance': 1, 'country': 1, 'course': 1, 'created': 1, 'decision': 1, 'enough': 1, 'even': 1, 'everything': 1, 'extreme': 1, 'finding': 1, 'first': 1, 'frankly': 1, 'fresh': 1, 'friend': 1, 'give': 1, 'giving': 1, 'go': 1, 'government': 1, 'great': 1, 'hand': 1, 'hit': 1, 'iraq': 1, 'job': 1, 'keep': 1, 'kept': 1, 'kind': 1, 'least': 1, 'listen': 1, 'lobbyist': 1, 'make': 1, 'may': 1, 'miss': 1, 'much': 1, 'niche': 1, 'nothing': 1, 'okay': 1, 'old': 1, 'opinion': 1, 'opinionated': 1, 'otherwise': 1, 'particularly': 1, 'personal': 1, 'perspective': 1, 'point': 1, 'position': 1, 'publish': 1, 'put': 1, 'quite': 1, 'reasoning': 1, 'requiring': 1, 'right': 1, 'situation': 1, 'sounding': 1, 'speaks': 1, 'stability': 1, 'support': 1, 'sure': 1, 'though': 1, 'thought': 1, 'told': 1, 'trust': 1, 'try': 1, 'war': 1, 'writes': 1}\n",
      "{'brain': 1, 'democrat': 1, 'hardcover': 1, 'republican': 1}\n"
     ]
    }
   ],
   "source": [
    "dict_review = get_dict_from_keyword_extracted(keyword_extracted_review)\n",
    "dict_title = get_dict_from_keyword_extracted(keyword_extracted_title)\n",
    "print(dict_review)\n",
    "print(dict_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.7707824197885758, 0.7707824197885758]\n",
      "0.885391209894288\n"
     ]
    }
   ],
   "source": [
    "# Similarities between keywords in title and review\n",
    "similarities = list()\n",
    "for word_title in dict_title.keys():\n",
    "    for word_review in dict_review.keys():\n",
    "        # Only calculate for word having a count of more than 1\n",
    "        if dict_review[word_review] > 1:\n",
    "            similarities.append(nlp(word_title).similarity(nlp(word_review)))\n",
    "# Sort similarities\n",
    "similarities.sort(reverse=True)\n",
    "average = sum(similarities[:len(dict_title)]) / len(similarities[:len(dict_title)])\n",
    "print(similarities[:len(dict_title)])\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score_feature(text):\n",
    "    \"\"\"!\n",
    "    @brief Get the sentiment score feature of a text using VADER sentiment analysis tool.\n",
    "    @param text (str): Text to be analyzed.\n",
    "    @return (dict): Sentiment score feature of the text.\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuation_feature(text):\n",
    "    \"\"\"!\n",
    "    @brief Get the punctuation feature of a text.\n",
    "    @param text (str): Text to be analyzed.\n",
    "    @return (list): List of punctuation counts normalized by the total count of punctuation in the text.\n",
    "                    [count of '.', count of '!', count of '?', count of ',']\n",
    "    \"\"\"\n",
    "    punctuations = ['.', '!', '?', ',']\n",
    "    punctuations_counts = list()\n",
    "    total_count = 0\n",
    "    for punctuation in punctuations:\n",
    "        count = text.count(punctuation)\n",
    "        punctuations_counts.append(count)\n",
    "        total_count += count\n",
    "    # Normalize the counts (ratio of punctuation count to total count)\n",
    "    if total_count != 0:\n",
    "        punctuations_counts = [count / total_count for count in punctuations_counts]\n",
    "    return punctuations_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS_feature(text, pipeline):\n",
    "    \"\"\"!\n",
    "    @brief Get the POS feature of a text.\n",
    "    @param text (str): Text to be analyzed.\n",
    "    @param pipeline (stanza.Pipeline): The Stanza pipeline use for the constituency parsing.\n",
    "    @return (list): List of POS tag counts normalized by the total count of POS tags in the text.\n",
    "                    [Noun count, Verb count, Adjective count, Adverb count]\n",
    "    \"\"\"\n",
    "    doc = pipeline(text)\n",
    "    POS_tags = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "    POS_counts = [0, 0, 0, 0]\n",
    "    total_count = 0\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            total_count += 1\n",
    "            if word.upos in POS_tags:\n",
    "                POS_counts[POS_tags.index(word.upos)] += 1\n",
    "    # Normalize the counts (ratio of POS tag count to total count)\n",
    "    if total_count != 0:\n",
    "        POS_counts = [count / total_count for count in POS_counts]\n",
    "\n",
    "    return POS_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_unigram_bigram_feature(text, vocabulary_sarcastic, vocabulary_regular, top_range):\n",
    "    \"\"\"!\n",
    "    @brief Get the word unigram and bigram feature of a text.\n",
    "    @param text (str): Text to be analyzed.\n",
    "    @param vocabulary_sarcastic (dict): Vocabulary of sarcastic words.\n",
    "    @param vocabulary_regular (dict): Vocabulary of regular words.\n",
    "    @param top_range (int): Number of top words to be considered for the feature.\n",
    "    @return (list): List of word unigram and bigram counts normalized by the total count of words in the text.\n",
    "                    [count of sarcastic words, count of regular words, count of sarcastic bigrams, count of regular bigrams]\n",
    "    \"\"\"\n",
    "    # Only consider the top words of the vocabulary\n",
    "    vocabulary_sarcastic = dict(sorted(vocabulary_sarcastic.items(), key=lambda item: item[1], reverse=True)[:top_range])\n",
    "    vocabulary_regular = dict(sorted(vocabulary_regular.items(), key=lambda item: item[1], reverse=True)[:top_range])\n",
    "    # Get the word unigram and bigram counts\n",
    "    word_unigram_bigram_counts = [0, 0, 0, 0]\n",
    "    word_unigram_bigram_counts[0] = sum([text.count(word) for word in vocabulary_sarcastic.keys()])\n",
    "    word_unigram_bigram_counts[1] = sum([text.count(word) for word in vocabulary_regular.keys()])\n",
    "    word_unigram_bigram_counts[2] = sum([text.count(word) for word in vocabulary_sarcastic.keys() if len(word.split()) == 2])\n",
    "    word_unigram_bigram_counts[3] = sum([text.count(word) for word in vocabulary_regular.keys() if len(word.split()) == 2])\n",
    "    # Normalize the counts (ratio of word unigram and bigram count to total count)\n",
    "    total_count = sum(word_unigram_bigram_counts)\n",
    "    if total_count != 0:\n",
    "        word_unigram_bigram_counts = [count / total_count for count in word_unigram_bigram_counts]\n",
    "    return word_unigram_bigram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_feature(text, sentiment_score, review_stars):\n",
    "    \"\"\"!\n",
    "    @brief Get the contextual feature of a text.\n",
    "    A sarcastic text may have a sentiment score that contradicts the review stars.\n",
    "    @param text (str): Text to be analyzed.\n",
    "    @param sentiment_score (float): Sentiment score of the text.\n",
    "    @param review_stars (float): Review stars of the text.\n",
    "    @return (float): Absolute difference between the sentiment score (normalized) and review stars.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize sentiment_score on a 0 to 5 scale (scale of review_stars)\n",
    "    # Sentiment score is in the range [-1, 1]\n",
    "    sentiment_score = (sentiment_score + 1) * 2.5\n",
    "    diff = abs(sentiment_score - review_stars)\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_feature(review, title, pipeline):\n",
    "    \"\"\"!\n",
    "    @brief Get the similarity feature of a review and a title.\n",
    "    @param review (str): Review to be analyzed.\n",
    "    @param title (str): Title to be analyzed.\n",
    "    @param pipeline (spacy.lang.en.English): The Spacy pipeline use for the similarity analysis.\n",
    "    @return (float): Average similarity between the review and the title.\n",
    "    \"\"\"\n",
    "    doc_review = pipeline(review)\n",
    "    doc_title = pipeline(title)\n",
    "    similarity = 0\n",
    "    for token_review in doc_review:\n",
    "        for token_title in doc_title:\n",
    "            similarity += token_review.similarity(token_title)\n",
    "    return similarity / (len(doc_review) * len(doc_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7269\n",
      "[0, 0, 0, 0]\n",
      "[0.5, 0.125, 0.25, 0.0]\n",
      "[0.5, 0.5, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiment_score_feature(dataset[\"sentence\"][0]))\n",
    "print(get_punctuation_feature(dataset[\"sentence\"][0]))\n",
    "print(get_POS_feature(dataset[\"sentence\"][0], constituency_parser))\n",
    "print(get_word_unigram_bigram_feature(dataset[\"sentence\"][0], vocabulary_sarcastic, vocabulary_regular, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
