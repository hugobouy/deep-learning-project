{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm detection with BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tunning on a combination of datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  is_sarcastic\n",
       "0  thirtysomething scientists unveil doomsday clo...           1.0\n",
       "1  dem rep. totally nails why congress is falling...           0.0\n",
       "2  eat your veggies: 9 deliciously different recipes           0.0\n",
       "3  inclement weather prevents liar from getting t...           1.0\n",
       "4  mother comes pretty close to using word 'strea...           1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "combined_df_file_path = \"../datasets/combined.parquet\"\n",
    "combined_df = pd.read_parquet(combined_df_file_path)\n",
    "\n",
    "# Display the first few rows of the dataset for a quick overview\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some statistics and cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sentence        0\n",
       " is_sarcastic    0\n",
       " dtype: int64,\n",
       " is_sarcastic\n",
       " 0.0    0.521391\n",
       " 1.0    0.478609\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any null values in the dataset\n",
    "combined_df_null_check = combined_df.isnull().sum()\n",
    "\n",
    "# Data cleaning: removing special characters and escape sequences from the sentences\n",
    "combined_df[\"sentence\"] = combined_df[\"sentence\"].apply(\n",
    "    lambda x: re.sub(r\"[\\n\\r\\t]+\", \" \", x)\n",
    ")\n",
    "\n",
    "# Checking the distribution of the 'is_sarcastic' column\n",
    "combined_df_label_distribution = combined_df[\"is_sarcastic\"].value_counts(\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "combined_df_null_check, combined_df_label_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28322, 6069, 6070)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into training, validation, and testing sets\n",
    "combined_train_data, combined_test_data = train_test_split(\n",
    "    combined_df, test_size=0.3, random_state=42\n",
    ")\n",
    "combined_val_data, combined_test_data = train_test_split(\n",
    "    combined_test_data, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Showing the size of each split\n",
    "combined_train_size, combined_val_size, combined_test_size = (\n",
    "    len(combined_train_data),\n",
    "    len(combined_val_data),\n",
    "    len(combined_test_data),\n",
    ")\n",
    "combined_train_size, combined_val_size, combined_test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset class for the BertTokenizer & PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remikalbe/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasticSentencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for the sarcastic sentences dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Encoding the sentences using the tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"sentence\": sentence,\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['unusually level-headed, charismatic lichen species named after obama',\n",
       "  'What does a mutation have to do with a heart transplant? What are you smoking?',\n",
       "  \"how to get the love that you 'deserve' in marriage\",\n",
       "  'pope francis beats confession out of uncooperative catholic',\n",
       "  'man bragging about how infrequently he receives dental care',\n",
       "  'english teacher already armed with deadly weapon called shakespeare',\n",
       "  'mitch mcconnell admits zika legislation is not clean',\n",
       "  \"giuliani: 'let's just start everything over'\",\n",
       "  \"'it's simply bursting with creative wonder,' says reviewer of new game where mario sometimes dresses as chef\",\n",
       "  \"student fills in new essay portion of sat with all c's\",\n",
       "  \"hillary clinton steals the show with pitch-perfect cameo in 'song for women 2017'\",\n",
       "  'paula poundstone still famous',\n",
       "  'man recalls desperate, exhausting 14-month job search that made him want to get into sales',\n",
       "  'man rides a horse into taco bell, and the internet is freaking out',\n",
       "  \"hear social media obsessed teen's reaction when her parents take away phone\",\n",
       "  'police have no idea how laquan mcdonald footage vanished right after they watched it'],\n",
       " 'input_ids': tensor([[  101, 12890,  2504,  ...,     0,     0,     0],\n",
       "         [  101,  2054,  2515,  ...,     0,     0,     0],\n",
       "         [  101,  2129,  2000,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2158, 12271,  ...,     0,     0,     0],\n",
       "         [  101,  2963,  2591,  ...,     0,     0,     0],\n",
       "         [  101,  2610,  2031,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "bert_base_uncased_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Constants\n",
    "COMBINED_MAX_LEN = combined_df[\"review\"].str.len().max()\n",
    "COMBINED_BATCH_SIZE = 16\n",
    "\n",
    "# Creating instances of the SarcasticSentencesDataset\n",
    "combined_train_dataset = SarcasticSentencesDataset(\n",
    "    combined_train_data[\"sentence\"].to_numpy(),\n",
    "    combined_train_data[\"is_sarcastic\"].to_numpy(),\n",
    "    bert_base_uncased_tokenizer,\n",
    "    COMBINED_MAX_LEN,\n",
    ")\n",
    "\n",
    "combined_val_dataset = SarcasticSentencesDataset(\n",
    "    combined_val_data[\"sentence\"].to_numpy(),\n",
    "    combined_val_data[\"is_sarcastic\"].to_numpy(),\n",
    "    bert_base_uncased_tokenizer,\n",
    "    COMBINED_MAX_LEN,\n",
    ")\n",
    "\n",
    "combined_test_dataset = SarcasticSentencesDataset(\n",
    "    combined_test_data[\"sentence\"].to_numpy(),\n",
    "    combined_test_data[\"is_sarcastic\"].to_numpy(),\n",
    "    bert_base_uncased_tokenizer,\n",
    "    COMBINED_MAX_LEN,\n",
    ")\n",
    "\n",
    "# Creating the DataLoaders for training, validation, and testing\n",
    "combined_train_loader = DataLoader(\n",
    "    combined_train_dataset, batch_size=COMBINED_BATCH_SIZE, shuffle=True\n",
    ")\n",
    "combined_val_loader = DataLoader(combined_val_dataset, batch_size=COMBINED_BATCH_SIZE)\n",
    "combined_test_loader = DataLoader(combined_test_dataset, batch_size=COMBINED_BATCH_SIZE)\n",
    "\n",
    "# Checking the first batch from the train_loader\n",
    "next(iter(combined_train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the train and validation loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler, LambdaLR\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# Typing imports\n",
    "from typing import Dict, Optional, List, Union\n",
    "\n",
    "# Other libraries\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: torch.device = torch.device(\n",
    "    device=\"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: Module,\n",
    "    data_loader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    device: torch.device,\n",
    "    scheduler: Union[_LRScheduler, LambdaLR],\n",
    "    loss_fn: CrossEntropyLoss,\n",
    "    n_examples: int,\n",
    "    feature_keys: Optional[List[str]] = None,  # List of keys if present\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = torch.Tensor([0]).to(device)\n",
    "\n",
    "    # For calculating precision and recall\n",
    "    tp_sarcasm = 0\n",
    "    tn_non_sarcasm = 0\n",
    "    fp_sarcasm = 0\n",
    "    fn_sarcasm = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        # Process inputs for single/multiple features\n",
    "        inputs = (\n",
    "            {key: batch[key].to(device) for key in feature_keys}\n",
    "            if feature_keys\n",
    "            else {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            }\n",
    "        )\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "        # Update TP, TN, FP, FN counters\n",
    "        tp_sarcasm += (preds & labels).sum().item()\n",
    "        tn_non_sarcasm += ((~preds.byte()) & (~labels.byte())).sum().item()\n",
    "        fp_sarcasm += (preds & (~labels.byte())).sum().item()\n",
    "        fn_sarcasm += ((~preds.byte()) & labels).sum().item()\n",
    "\n",
    "    # Calculate precision and recall for sarcasm class\n",
    "    precision_sarcasm = (\n",
    "        tp_sarcasm / (tp_sarcasm + fp_sarcasm) if (tp_sarcasm + fp_sarcasm) > 0 else 0\n",
    "    )\n",
    "    recall_sarcasm = (\n",
    "        tp_sarcasm / (tp_sarcasm + fn_sarcasm) if (tp_sarcasm + fn_sarcasm) > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Calculate precision and recall for non-sarcasm class\n",
    "    precision_non_sarcasm = (\n",
    "        tn_non_sarcasm / (tn_non_sarcasm + fn_sarcasm)\n",
    "        if (tn_non_sarcasm + fn_sarcasm) > 0\n",
    "        else 0\n",
    "    )\n",
    "    recall_non_sarcasm = (\n",
    "        tn_non_sarcasm / (tn_non_sarcasm + fp_sarcasm)\n",
    "        if (tn_non_sarcasm + fp_sarcasm) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": correct_predictions.float().item() / n_examples,\n",
    "        \"precision_sarcasm\": precision_sarcasm,\n",
    "        \"recall_sarcasm\": recall_sarcasm,\n",
    "        \"precision_non_sarcasm\": precision_non_sarcasm,\n",
    "        \"recall_non_sarcasm\": recall_non_sarcasm,\n",
    "        \"loss\": np.mean(losses),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    model: Module,\n",
    "    data_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    loss_fn: CrossEntropyLoss,\n",
    "    n_examples: int,\n",
    "    feature_keys: Optional[List[str]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = torch.Tensor([0]).to(device)\n",
    "\n",
    "    # Initialize counters for precision and recall\n",
    "    tp_sarcasm = 0\n",
    "    tn_non_sarcasm = 0\n",
    "    fp_sarcasm = 0\n",
    "    fn_sarcasm = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "            # Process inputs for single/multiple features\n",
    "            inputs = (\n",
    "                {key: batch[key].to(device) for key in feature_keys}\n",
    "                if feature_keys\n",
    "                else {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                }\n",
    "            )\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "            # Update TP, TN, FP, FN counters for precision and recall calculations\n",
    "            tp_sarcasm += (preds & labels).sum().item()\n",
    "            tn_non_sarcasm += ((~preds.byte()) & (~labels.byte())).sum().item()\n",
    "            fp_sarcasm += (preds & (~labels.byte())).sum().item()\n",
    "            fn_sarcasm += ((~preds.byte()) & labels).sum().item()\n",
    "\n",
    "    # Calculate precision and recall for sarcasm class\n",
    "    precision_sarcasm = (\n",
    "        tp_sarcasm / (tp_sarcasm + fp_sarcasm) if (tp_sarcasm + fp_sarcasm) > 0 else 0\n",
    "    )\n",
    "    recall_sarcasm = (\n",
    "        tp_sarcasm / (tp_sarcasm + fn_sarcasm) if (tp_sarcasm + fn_sarcasm) > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Calculate precision and recall for non-sarcasm class\n",
    "    precision_non_sarcasm = (\n",
    "        tn_non_sarcasm / (tn_non_sarcasm + fn_sarcasm)\n",
    "        if (tn_non_sarcasm + fn_sarcasm) > 0\n",
    "        else 0\n",
    "    )\n",
    "    recall_non_sarcasm = (\n",
    "        tn_non_sarcasm / (tn_non_sarcasm + fp_sarcasm)\n",
    "        if (tn_non_sarcasm + fp_sarcasm) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": correct_predictions.float().item() / n_examples,\n",
    "        \"precision_sarcasm\": precision_sarcasm,\n",
    "        \"recall_sarcasm\": recall_sarcasm,\n",
    "        \"precision_non_sarcasm\": precision_non_sarcasm,\n",
    "        \"recall_non_sarcasm\": recall_non_sarcasm,\n",
    "        \"loss\": np.mean(losses),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & evaluation of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the train and validation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    scheduler: Union[_LRScheduler, LambdaLR],\n",
    "    loss_fn: CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    train_dataset_len: int,\n",
    "    val_dataset_len: int,\n",
    "    feature_keys: Optional[List[str]] = None,\n",
    "):\n",
    "    best_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Training phase\n",
    "        train_output = train_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            scheduler=scheduler,\n",
    "            loss_fn=loss_fn,\n",
    "            n_examples=train_dataset_len,\n",
    "            feature_keys=feature_keys,\n",
    "        )\n",
    "\n",
    "        train_metrics_str = \" | \".join(\n",
    "            f\"{metric}: {value:.4f}\" for metric, value in train_output.items()\n",
    "        )\n",
    "        print(f\"Training Metrics: {train_metrics_str}\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_output = eval_model(\n",
    "            model=model,\n",
    "            data_loader=val_loader,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn,\n",
    "            n_examples=val_dataset_len,\n",
    "            feature_keys=feature_keys,\n",
    "        )\n",
    "\n",
    "        val_metrics_str = \" | \".join(\n",
    "            f\"{metric}: {value:.4f}\" for metric, value in val_output.items()\n",
    "        )\n",
    "        print(f\"Validation Metrics: {val_metrics_str}\")\n",
    "\n",
    "        # Example: Save the best model based on validation accuracy\n",
    "        if val_output[\"accuracy\"] > best_accuracy:\n",
    "            best_accuracy = val_output[\"accuracy\"]\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Saved Best Model!\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.4f} on Epoch {best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/remikalbe/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1771/1771 [09:58<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Loss: 0.4146 | Accuracy: 0.8031 | Sarcasm Precision: 0.7991 | Sarcasm Recall: 0.7860 | Non-Sarcasm Precision: 0.9996 | Non-Sarcasm Recall: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [00:40<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "Loss: 0.3539 | Accuracy: 0.8423 | Sarcasm Precision: 0.8892 | Sarcasm Recall: 0.7693 | Non-Sarcasm Precision: 0.9996 | Non-Sarcasm Recall: 0.9998\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1771/1771 [11:04<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Loss: 0.2959 | Accuracy: 0.8718 | Sarcasm Precision: 0.8616 | Sarcasm Recall: 0.8721 | Non-Sarcasm Precision: 0.9998 | Non-Sarcasm Recall: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [00:35<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "Loss: 0.3605 | Accuracy: 0.8433 | Sarcasm Precision: 0.9149 | Sarcasm Recall: 0.7447 | Non-Sarcasm Precision: 0.9995 | Non-Sarcasm Recall: 0.9999\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1771/1771 [11:48<00:00,  2.50it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Loss: 0.2452 | Accuracy: 0.8939 | Sarcasm Precision: 0.8832 | Sarcasm Recall: 0.8969 | Non-Sarcasm Precision: 0.9998 | Non-Sarcasm Recall: 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [00:29<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "Loss: 0.3809 | Accuracy: 0.8446 | Sarcasm Precision: 0.9183 | Sarcasm Recall: 0.7444 | Non-Sarcasm Precision: 0.9995 | Non-Sarcasm Recall: 0.9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "COMBINED_PRETRAINED_MODEL_NAME_OR_PATH = \"bert-base-uncased\"\n",
    "COMBINED_NUM_LABELS = 2  # Number of labels in the dataset\n",
    "COMBINED_HIDDEN_DROPOUT_PROB = 0.3  # Dropout rate\n",
    "COMBINED_ATTENTION_PROBS_DROPOUT_PROB = 0.3  # Dropout rate in attention heads\n",
    "COMBINED_NUM_EPOCHS = 3  # Number of epochs\n",
    "COMBINED_LR = 2e-5  # Learning rate\n",
    "COMBINED_WEIGHT_DECAY = 0.01  # Weight decay for regularization\n",
    "COMBINED_NUM_WARMUP_STEPS = 0  # Number of warmup steps for learning rate scheduler\n",
    "\n",
    "# Load pre-trained model\n",
    "combined_model = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=COMBINED_PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    num_labels=COMBINED_NUM_LABELS,\n",
    "    hidden_dropout_prob=COMBINED_HIDDEN_DROPOUT_PROB,  # dropout rate,\n",
    "    attention_probs_dropout_prob=COMBINED_ATTENTION_PROBS_DROPOUT_PROB,  # dropout rate in attention heads\n",
    ")\n",
    "\n",
    "# For typing purposes, check if model is an instance of Module\n",
    "if not isinstance(combined_model, Module):\n",
    "    raise ValueError(\"Model must be an instance of Module\")\n",
    "\n",
    "# Send the model to GPU if available\n",
    "combined_model.to(device=device)  # type: ignore\n",
    "\n",
    "# Optimizer\n",
    "combined_optimizer = AdamW(\n",
    "    combined_model.parameters(), lr=COMBINED_LR, weight_decay=COMBINED_WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Total number of training steps\n",
    "combined_total_steps = len(combined_train_loader) * COMBINED_NUM_EPOCHS\n",
    "\n",
    "# Scheduler for learning rate\n",
    "combined_scheduler = get_linear_schedule_with_warmup(\n",
    "    combined_optimizer,\n",
    "    num_warmup_steps=COMBINED_NUM_WARMUP_STEPS,\n",
    "    num_training_steps=combined_total_steps,\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "combined_loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Feature keys\n",
    "combined_feature_keys = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_and_evaluate(\n",
    "    model=combined_model,\n",
    "    train_loader=combined_train_loader,\n",
    "    val_loader=combined_val_loader,\n",
    "    optimizer=combined_optimizer,\n",
    "    scheduler=combined_scheduler,\n",
    "    loss_fn=combined_loss_fn,\n",
    "    device=device,\n",
    "    num_epochs=COMBINED_NUM_EPOCHS,\n",
    "    train_dataset_len=len(combined_train_dataset),\n",
    "    val_dataset_len=len(combined_val_dataset),\n",
    "    feature_keys=combined_feature_keys,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "FINE_TUNED_BERT_PATH = \"sarcastic_model.pth\"\n",
    "torch.save(combined_model.state_dict(), FINE_TUNED_BERT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new model based on the pre-trained BERT model, adding review features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset class for the new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasticProductReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for sarcastic product reviews with multiple text features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review_data = self.data.iloc[idx]\n",
    "        label = review_data[\"is_sarcastic\"]\n",
    "\n",
    "        # Tokenizing each text feature separately\n",
    "        title_encoding = self.tokenize_text_feature(review_data[\"title\"])\n",
    "        author_encoding = self.tokenize_text_feature(review_data[\"author\"])\n",
    "        product_encoding = self.tokenize_text_feature(review_data[\"product\"])\n",
    "        review_encoding = self.tokenize_text_feature(review_data[\"review\"])\n",
    "\n",
    "        # Convert stars rating to a tensor\n",
    "        stars_rating = torch.tensor([float(review_data[\"stars\"])], dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "            \"title_input_ids\": title_encoding[\"input_ids\"].flatten(),\n",
    "            \"title_attention_mask\": title_encoding[\"attention_mask\"].flatten(),\n",
    "            \"author_input_ids\": author_encoding[\"input_ids\"].flatten(),\n",
    "            \"author_attention_mask\": author_encoding[\"attention_mask\"].flatten(),\n",
    "            \"product_input_ids\": product_encoding[\"input_ids\"].flatten(),\n",
    "            \"product_attention_mask\": product_encoding[\"attention_mask\"].flatten(),\n",
    "            \"review_input_ids\": review_encoding[\"input_ids\"].flatten(),\n",
    "            \"review_attention_mask\": review_encoding[\"attention_mask\"].flatten(),\n",
    "            \"stars\": stars_rating.flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def tokenize_text_feature(self, text):\n",
    "        return self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_len,  # truncate or pad to max_len\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",  # pad to max_length\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",  # return tensors for PyTorch\n",
    "            truncation=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Dropout, ReLU\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOutput:\n",
    "    def __init__(self, logits):\n",
    "        self.logits = logits\n",
    "\n",
    "\n",
    "class ExtendedBertForMultiFeatureClassification(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_bert_path,\n",
    "        fine_tuned_bert_path,\n",
    "        hidden_size,\n",
    "        num_labels,\n",
    "        hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob,\n",
    "        classifier_dropout_prob,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            pretrained_model_name_or_path=pretrained_bert_path,\n",
    "            num_labels=num_labels,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,  # dropout rate,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,  # dropout rate in attention heads\n",
    "        )\n",
    "        if not isinstance(self.bert, Module):\n",
    "            raise ValueError(\"Model must be an instance of Module\")\n",
    "\n",
    "        # Load the state_dict from the saved model\n",
    "        state_dict = torch.load(fine_tuned_bert_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        # Remove the keys related to the classification head\n",
    "        state_dict = {\n",
    "            key: value\n",
    "            for key, value in state_dict.items()\n",
    "            if not key.startswith(\"classifier.\")\n",
    "        }\n",
    "\n",
    "        # Update the state with the weighed from the fine-tuned model (excluding classifier weights)\n",
    "        self.bert.load_state_dict(\n",
    "            state_dict, strict=False\n",
    "        )  # Set strict to False to ignore missing keys\n",
    "\n",
    "        # Assuming features for title, author, product, review\n",
    "        num_bert_features = 4  # how many BERT-encoded text features we're combining\n",
    "        num_additional_features = 1  # e.g., stars\n",
    "\n",
    "        # The feature combiner layer to merge BERT-encoded features\n",
    "        self.feature_combiner = Linear(\n",
    "            self.bert.config.hidden_size * num_bert_features, hidden_size\n",
    "        )\n",
    "        self.feature_combiner_activation = ReLU()\n",
    "\n",
    "        # The classifier head that includes an additional hidden layer\n",
    "        self.classifier_hidden = Linear(\n",
    "            hidden_size + num_additional_features, hidden_size\n",
    "        )\n",
    "        self.classifier_hidden_activation = ReLU()\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = Linear(hidden_size, num_labels)\n",
    "        self.dropout = Dropout(classifier_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        title_input_ids,\n",
    "        title_attention_mask,\n",
    "        author_input_ids,\n",
    "        author_attention_mask,\n",
    "        product_input_ids,\n",
    "        product_attention_mask,\n",
    "        review_input_ids,\n",
    "        review_attention_mask,\n",
    "        stars,\n",
    "    ):\n",
    "        if not isinstance(self.bert, Module):\n",
    "            raise ValueError(\"Model must be an instance of Module\")\n",
    "\n",
    "        # Process each text input through the fine-tuned BERT independently\n",
    "        # Extract the last hidden state of the [CLS] token from each output\n",
    "        title_cls = self.bert(\n",
    "            title_input_ids, attention_mask=title_attention_mask\n",
    "        ).pooler_output\n",
    "        author_cls = self.bert(\n",
    "            author_input_ids, attention_mask=author_attention_mask\n",
    "        ).pooler_output\n",
    "        product_cls = self.bert(\n",
    "            product_input_ids, attention_mask=product_attention_mask\n",
    "        ).pooler_output\n",
    "        review_cls = self.bert(\n",
    "            review_input_ids, attention_mask=review_attention_mask\n",
    "        ).pooler_output\n",
    "\n",
    "        # Combine [CLS] token outputs for all text features\n",
    "        combined_cls = torch.cat(\n",
    "            (\n",
    "                title_cls,  # Should be (batch_size, hidden_size)\n",
    "                author_cls,  # Should be (batch_size, hidden_size)\n",
    "                product_cls,  # Should be (batch_size, hidden_size)\n",
    "                review_cls,  # Should be (batch_size, hidden_size)\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Make sure stars is 2D with shape (batch_size, 1)\n",
    "        stars = stars.unsqueeze(1) if stars.dim() == 1 else stars\n",
    "\n",
    "        # Apply dropout and pass through the affine transformation and activation\n",
    "        combined_features = self.dropout(\n",
    "            self.feature_combiner_activation(self.feature_combiner(combined_cls))\n",
    "        )\n",
    "\n",
    "        # Combine with the additional feature (e.g., stars)\n",
    "        combined_with_additional_feature = torch.cat((combined_features, stars), dim=1)\n",
    "\n",
    "        # Pass through the second hidden layer\n",
    "        classifier_hidden_output = self.dropout(\n",
    "            self.classifier_hidden_activation(\n",
    "                self.classifier_hidden(combined_with_additional_feature)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Final classifier to get logits\n",
    "        logits = self.classifier(classifier_hidden_output)\n",
    "\n",
    "        return ModelOutput(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>product</th>\n",
       "      <th>review</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Listening to this \"Hurt\" me!</td>\n",
       "      <td>November 8, 2007</td>\n",
       "      <td>MomKKC \"momkkc\"</td>\n",
       "      <td>The Sun Also Rises (Audio CD)</td>\n",
       "      <td>William Hurt cannot read.  At all.  The cadenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>40% price hike, hmm</td>\n",
       "      <td>April 15, 2010</td>\n",
       "      <td>M. Barnhart</td>\n",
       "      <td>Heineken BT06 BeerTender Tubes, Pack of 6 (Kit...</td>\n",
       "      <td>As another reviewer noted, these used to be 10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Don't Mess With the Lupine Trinity!!!</td>\n",
       "      <td>June 2, 2010</td>\n",
       "      <td>Jake &amp;#34;The Wolfman&amp;#34; Sanchez</td>\n",
       "      <td>The Mountain Three Wolf Moon Short Sleeve Tee ...</td>\n",
       "      <td>I've read several reviews from people who have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>IT'S A BLENDER!</td>\n",
       "      <td>June 17, 2010</td>\n",
       "      <td>S. Cashdollar</td>\n",
       "      <td>Margaritaville DM1000 Frozen Concoction Maker ...</td>\n",
       "      <td>If you pay $250 for this blender you need your...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Another movie to ignore....</td>\n",
       "      <td>April 24, 2010</td>\n",
       "      <td>Kody \"ParisHiltonFan\"</td>\n",
       "      <td>Valentine's Day (DVD)</td>\n",
       "      <td>A perfect date movie: you'll miss absolutely n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stars                                  title              date  \\\n",
       "0   1.0           Listening to this \"Hurt\" me!  November 8, 2007   \n",
       "1   1.0                    40% price hike, hmm    April 15, 2010   \n",
       "2   5.0  Don't Mess With the Lupine Trinity!!!      June 2, 2010   \n",
       "3   1.0                        IT'S A BLENDER!     June 17, 2010   \n",
       "4   1.0            Another movie to ignore....    April 24, 2010   \n",
       "\n",
       "                               author  \\\n",
       "0                     MomKKC \"momkkc\"   \n",
       "1                         M. Barnhart   \n",
       "2  Jake &#34;The Wolfman&#34; Sanchez   \n",
       "3                       S. Cashdollar   \n",
       "4               Kody \"ParisHiltonFan\"   \n",
       "\n",
       "                                             product  \\\n",
       "0                      The Sun Also Rises (Audio CD)   \n",
       "1  Heineken BT06 BeerTender Tubes, Pack of 6 (Kit...   \n",
       "2  The Mountain Three Wolf Moon Short Sleeve Tee ...   \n",
       "3  Margaritaville DM1000 Frozen Concoction Maker ...   \n",
       "4                              Valentine's Day (DVD)   \n",
       "\n",
       "                                              review  is_sarcastic  \n",
       "0  William Hurt cannot read.  At all.  The cadenc...             1  \n",
       "1  As another reviewer noted, these used to be 10...             1  \n",
       "2  I've read several reviews from people who have...             1  \n",
       "3  If you pay $250 for this blender you need your...             1  \n",
       "4  A perfect date movie: you'll miss absolutely n...             1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "amz_combined_file_path = \"../datasets/amazon_combined.parquet\"\n",
    "amz_combined_df = pd.read_parquet(amz_combined_file_path)\n",
    "\n",
    "# Display the first few rows of the dataset for a quick overview\n",
    "amz_combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(stars           0\n",
       " title           0\n",
       " date            0\n",
       " author          0\n",
       " product         0\n",
       " review          0\n",
       " is_sarcastic    0\n",
       " dtype: int64,\n",
       " is_sarcastic\n",
       " 0    0.651515\n",
       " 1    0.348485\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning: removing special characters and escape sequences from the sentences\n",
    "amz_combined_df[\"review\"] = amz_combined_df[\"review\"].apply(\n",
    "    lambda x: re.sub(r\"[\\n\\r\\t]+\", \" \", x)\n",
    ")\n",
    "amz_combined_df[\"product\"] = amz_combined_df[\"product\"].apply(\n",
    "    lambda x: re.sub(r\"[\\n\\r\\t]+\", \" \", x)\n",
    ")\n",
    "amz_combined_df[\"author\"] = amz_combined_df[\"author\"].apply(\n",
    "    lambda x: re.sub(r\"[\\n\\r\\t]+\", \" \", x)\n",
    ")\n",
    "amz_combined_df[\"title\"] = amz_combined_df[\"title\"].apply(\n",
    "    lambda x: re.sub(r\"[\\n\\r\\t]+\", \" \", x)\n",
    ")\n",
    "\n",
    "# Checking for any null values in the dataset\n",
    "amz_combined_null_check = amz_combined_df.isnull().sum()\n",
    "\n",
    "# Checking the distribution of the 'is_sarcastic' column\n",
    "amz_combined_label_distribution = amz_combined_df[\"is_sarcastic\"].value_counts(\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "amz_combined_null_check, amz_combined_label_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877, 188, 189)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into training, validation, and testing sets\n",
    "amz_combined_train_data, amz_combined_test_data = train_test_split(\n",
    "    amz_combined_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=amz_combined_df[\"is_sarcastic\"],  # Use the labels for stratification\n",
    ")\n",
    "amz_combined_val_data, amz_combined_test_data = train_test_split(\n",
    "    amz_combined_test_data,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=amz_combined_test_data[\n",
    "        \"is_sarcastic\"\n",
    "    ],  # Use the labels for stratification\n",
    ")\n",
    "\n",
    "# Showing the size of each split\n",
    "amz_combined_train_size, amz_combined_val_size, amz_combined_test_size = (\n",
    "    len(amz_combined_train_data),\n",
    "    len(amz_combined_val_data),\n",
    "    len(amz_combined_test_data),\n",
    ")\n",
    "amz_combined_train_size, amz_combined_val_size, amz_combined_test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate the dataset class & data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for DataLoader\n",
    "AMZ_COMBINED_MAX_LEN = COMBINED_MAX_LEN\n",
    "AMZ_COMBINED_BATCH_SIZE = 16\n",
    "\n",
    "# Compute class weights inverse proportional to class frequencies\n",
    "class_sample_counts = amz_combined_train_data[\"is_sarcastic\"].value_counts()\n",
    "class_weights = 1.0 / class_sample_counts\n",
    "weights = amz_combined_train_data[\"is_sarcastic\"].map(class_weights)\n",
    "weights = weights.to_numpy()\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "# Instantiate the custom Dataset for Amazon product reviews\n",
    "amz_combined_train_dataset = SarcasticProductReviewDataset(\n",
    "    data=amz_combined_train_data,\n",
    "    tokenizer=bert_base_uncased_tokenizer,\n",
    "    max_len=AMZ_COMBINED_MAX_LEN,\n",
    ")\n",
    "\n",
    "amz_combined_val_dataset = SarcasticProductReviewDataset(\n",
    "    data=amz_combined_val_data,\n",
    "    tokenizer=bert_base_uncased_tokenizer,\n",
    "    max_len=AMZ_COMBINED_MAX_LEN,\n",
    ")\n",
    "\n",
    "amz_combined_test_dataset = SarcasticProductReviewDataset(\n",
    "    data=amz_combined_test_data,\n",
    "    tokenizer=bert_base_uncased_tokenizer,\n",
    "    max_len=AMZ_COMBINED_MAX_LEN,\n",
    ")\n",
    "\n",
    "# Create DataLoader instances for training, validation, and testing\n",
    "amz_combined_train_loader = DataLoader(\n",
    "    dataset=amz_combined_train_dataset,\n",
    "    batch_size=AMZ_COMBINED_BATCH_SIZE,\n",
    "    sampler=sampler,  # Use the WeightedRandomSampler instead of shuffle\n",
    ")\n",
    "\n",
    "amz_combined_val_loader = DataLoader(\n",
    "    dataset=amz_combined_val_dataset, batch_size=AMZ_COMBINED_BATCH_SIZE\n",
    ")\n",
    "\n",
    "amz_combined_test_loader = DataLoader(\n",
    "    dataset=amz_combined_test_dataset, batch_size=AMZ_COMBINED_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate the model & then necessary objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remikalbe/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "AMZ_COMBINED_PRETRAINED_MODEL_NAME_OR_PATH = COMBINED_PRETRAINED_MODEL_NAME_OR_PATH\n",
    "AMZ_COMBINED_NUM_LABELS = COMBINED_NUM_LABELS\n",
    "AMZ_COMBINED_HIDDEN_DROPOUT_PROB = 0.45\n",
    "AMZ_COMBINED_ATTENTION_PROBS_DROPOUT_PROB = COMBINED_ATTENTION_PROBS_DROPOUT_PROB\n",
    "AMZ_COMBINED_NUM_EPOCHS = 10\n",
    "AMZ_COMBINED_LR = COMBINED_LR\n",
    "AMZ_COMBINED_WEIGHT_DECAY = 0.05\n",
    "\n",
    "# Specific hyperparameters for the amz model\n",
    "AMZ_COMBINED_HIDDEN_SIZE = 768  # Default hidden size for BERT base\n",
    "AMZ_COMBINED_STAR_HIDDEN_DROPOUT_PROB = 0.3  # Dropout rate for the star rating feature\n",
    "\n",
    "# Path to the fine-tuned BERT model\n",
    "FINE_TUNED_BERT_PATH = \"sarcastic_model.pth\"\n",
    "\n",
    "# Instantiate the extended model\n",
    "amz_combined_model = ExtendedBertForMultiFeatureClassification(\n",
    "    pretrained_bert_path=AMZ_COMBINED_PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    fine_tuned_bert_path=FINE_TUNED_BERT_PATH,\n",
    "    hidden_size=AMZ_COMBINED_HIDDEN_SIZE,\n",
    "    num_labels=AMZ_COMBINED_NUM_LABELS,\n",
    "    hidden_dropout_prob=AMZ_COMBINED_HIDDEN_DROPOUT_PROB,\n",
    "    attention_probs_dropout_prob=AMZ_COMBINED_ATTENTION_PROBS_DROPOUT_PROB,\n",
    "    classifier_dropout_prob=AMZ_COMBINED_STAR_HIDDEN_DROPOUT_PROB,\n",
    ")\n",
    "\n",
    "# Send the model to GPU if available\n",
    "amz_combined_model.to(device)  # type: ignore\n",
    "\n",
    "# Optimizer\n",
    "amz_combined_optimizer = AdamW(\n",
    "    amz_combined_model.parameters(),\n",
    "    lr=AMZ_COMBINED_LR,\n",
    "    weight_decay=AMZ_COMBINED_WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Total number of training steps\n",
    "amz_combined_total_steps = len(amz_combined_train_loader) * AMZ_COMBINED_NUM_EPOCHS\n",
    "\n",
    "# Scheduler for learning rate\n",
    "amz_combined_scheduler = get_linear_schedule_with_warmup(\n",
    "    amz_combined_optimizer,\n",
    "    num_warmup_steps=COMBINED_NUM_WARMUP_STEPS,\n",
    "    num_training_steps=amz_combined_total_steps,\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "amz_combined_loss_fn = CrossEntropyLoss()\n",
    "\n",
    "amz_combined_feature_keys = [\n",
    "    \"title_input_ids\",\n",
    "    \"title_attention_mask\",\n",
    "    \"author_input_ids\",\n",
    "    \"author_attention_mask\",\n",
    "    \"product_input_ids\",\n",
    "    \"product_attention_mask\",\n",
    "    \"review_input_ids\",\n",
    "    \"review_attention_mask\",\n",
    "    \"stars\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (20509) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [16, 20509].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb Cell 44\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_and_evaluate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39;49mamz_combined_model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mamz_combined_train_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     val_loader\u001b[39m=\u001b[39;49mamz_combined_val_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mamz_combined_optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mamz_combined_scheduler,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mamz_combined_loss_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mAMZ_COMBINED_NUM_EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_dataset_len\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(amz_combined_train_dataset),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     val_dataset_len\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(amz_combined_val_dataset),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     feature_keys\u001b[39m=\u001b[39;49mamz_combined_feature_keys,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m AMZ_FINE_TUNED_BERT_PATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mamazon_sarcastic_model.pth\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb Cell 44\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Training phase\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_output \u001b[39m=\u001b[39m train_epoch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     data_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     n_examples\u001b[39m=\u001b[39;49mtrain_dataset_len,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     feature_keys\u001b[39m=\u001b[39;49mfeature_keys,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m train_metrics_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m metric, value \u001b[39min\u001b[39;00m train_output\u001b[39m.\u001b[39mitems()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Metrics: \u001b[39m\u001b[39m{\u001b[39;00mtrain_metrics_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb Cell 44\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, labels)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb Cell 44\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mModel must be an instance of Module\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Process each text input through the fine-tuned BERT independently\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# Extract the last hidden state of the [CLS] token from each output\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m title_cls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     title_input_ids, attention_mask\u001b[39m=\u001b[39;49mtitle_attention_mask\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m )\u001b[39m.\u001b[39mpooler_output\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m author_cls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     author_input_ids, attention_mask\u001b[39m=\u001b[39mauthor_attention_mask\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m )\u001b[39m.\u001b[39mpooler_output\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m product_cls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     product_input_ids, attention_mask\u001b[39m=\u001b[39mproduct_attention_mask\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/remikalbe/Git/github.com/IIT-DL-PROJECT/code/bert_model.ipynb#X55sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m )\u001b[39m.\u001b[39mpooler_output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/iit_dl_project/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings, \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    987\u001b[0m     buffered_token_type_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 988\u001b[0m     buffered_token_type_ids_expanded \u001b[39m=\u001b[39m buffered_token_type_ids\u001b[39m.\u001b[39;49mexpand(batch_size, seq_length)\n\u001b[1;32m    989\u001b[0m     token_type_ids \u001b[39m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    990\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (20509) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [16, 20509].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train_and_evaluate(\n",
    "    model=amz_combined_model,\n",
    "    train_loader=amz_combined_train_loader,\n",
    "    val_loader=amz_combined_val_loader,\n",
    "    optimizer=amz_combined_optimizer,\n",
    "    scheduler=amz_combined_scheduler,\n",
    "    loss_fn=amz_combined_loss_fn,\n",
    "    device=device,\n",
    "    num_epochs=AMZ_COMBINED_NUM_EPOCHS,\n",
    "    train_dataset_len=len(amz_combined_train_dataset),\n",
    "    val_dataset_len=len(amz_combined_val_dataset),\n",
    "    feature_keys=amz_combined_feature_keys,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "AMZ_FINE_TUNED_BERT_PATH = \"amazon_sarcastic_model.pth\"\n",
    "torch.save(amz_combined_model.state_dict(), AMZ_FINE_TUNED_BERT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
